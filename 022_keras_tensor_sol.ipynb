{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gGsMHgvKZVRO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_lu_r1ndZVRR"
      },
      "outputs": [],
      "source": [
        "# Helper to plot loss\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52niBYdKZVRS"
      },
      "source": [
        "# Keras, TensorFlow, and Neural Network Regression\n",
        "\n",
        "As we have seen, neural networks aren't quite as complex as they appear at first, however we still generally don't want to have to build them from scratch very often. The libraries that we will primarily use for creating neural network models are Tensorflow and Keras. \n",
        "\n",
        "### Tensorflow\n",
        "\n",
        "Tensorflow, developed by Google, is one of the most popular libraries for neural networks. \n",
        "\n",
        "### Keras\n",
        "\n",
        "Keras is another package that provides an an API offering an easier to use interface to Tensorflow, allowing us to use it with code that is higher level, avoiding much of the linear math that can make Tensorflow frustrating. Since its introduction Keras has been wrapped in with Tensorflow and the two are normally now blended together as far as we are concerned. \n",
        "\n",
        "### Other Alternatives\n",
        "\n",
        "Keras and Tensorflow are not the only libraries of neural networks, the primary competitor to Tensorflow is PyTorch, which was developed by Facebook. PyTorch does pretty much the same thing as Tensorflow, we won't look at it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JrKP3wO_ZVRT",
        "outputId": "1ac1822c-99a5-4e98-ea4d-0158aed55c65"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21608</th>\n",
              "      <td>360000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1530</td>\n",
              "      <td>1131</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1530</td>\n",
              "      <td>0</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>98103</td>\n",
              "      <td>47.6993</td>\n",
              "      <td>-122.346</td>\n",
              "      <td>1530</td>\n",
              "      <td>1509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21609</th>\n",
              "      <td>400000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2310</td>\n",
              "      <td>5813</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2310</td>\n",
              "      <td>0</td>\n",
              "      <td>2014</td>\n",
              "      <td>0</td>\n",
              "      <td>98146</td>\n",
              "      <td>47.5107</td>\n",
              "      <td>-122.362</td>\n",
              "      <td>1830</td>\n",
              "      <td>7200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21610</th>\n",
              "      <td>402101.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1020</td>\n",
              "      <td>1350</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1020</td>\n",
              "      <td>0</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>98144</td>\n",
              "      <td>47.5944</td>\n",
              "      <td>-122.299</td>\n",
              "      <td>1020</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21611</th>\n",
              "      <td>400000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1600</td>\n",
              "      <td>2388</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1600</td>\n",
              "      <td>0</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>98027</td>\n",
              "      <td>47.5345</td>\n",
              "      <td>-122.069</td>\n",
              "      <td>1410</td>\n",
              "      <td>1287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21612</th>\n",
              "      <td>325000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1020</td>\n",
              "      <td>1076</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1020</td>\n",
              "      <td>0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>98144</td>\n",
              "      <td>47.5941</td>\n",
              "      <td>-122.299</td>\n",
              "      <td>1020</td>\n",
              "      <td>1357</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
              "21608  360000.0         3       2.50         1530      1131     3.0   \n",
              "21609  400000.0         4       2.50         2310      5813     2.0   \n",
              "21610  402101.0         2       0.75         1020      1350     2.0   \n",
              "21611  400000.0         3       2.50         1600      2388     2.0   \n",
              "21612  325000.0         2       0.75         1020      1076     2.0   \n",
              "\n",
              "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
              "21608           0     0          3      8        1530              0   \n",
              "21609           0     0          3      8        2310              0   \n",
              "21610           0     0          3      7        1020              0   \n",
              "21611           0     0          3      8        1600              0   \n",
              "21612           0     0          3      7        1020              0   \n",
              "\n",
              "       yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
              "21608      2009             0    98103  47.6993 -122.346           1530   \n",
              "21609      2014             0    98146  47.5107 -122.362           1830   \n",
              "21610      2009             0    98144  47.5944 -122.299           1020   \n",
              "21611      2004             0    98027  47.5345 -122.069           1410   \n",
              "21612      2008             0    98144  47.5941 -122.299           1020   \n",
              "\n",
              "       sqft_lot15  \n",
              "21608        1509  \n",
              "21609        7200  \n",
              "21610        2007  \n",
              "21611        1287  \n",
              "21612        1357  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"data/house_data.csv\")\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uLE617U8ZVRV",
        "outputId": "08d0318a-9c52-4e72-b585-ade094db2591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21613 entries, 0 to 21612\n",
            "Data columns (total 19 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   price          21613 non-null  float64\n",
            " 1   bedrooms       21613 non-null  int64  \n",
            " 2   bathrooms      21613 non-null  float64\n",
            " 3   sqft_living    21613 non-null  int64  \n",
            " 4   sqft_lot       21613 non-null  int64  \n",
            " 5   floors         21613 non-null  float64\n",
            " 6   waterfront     21613 non-null  int64  \n",
            " 7   view           21613 non-null  int64  \n",
            " 8   condition      21613 non-null  int64  \n",
            " 9   grade          21613 non-null  int64  \n",
            " 10  sqft_above     21613 non-null  int64  \n",
            " 11  sqft_basement  21613 non-null  int64  \n",
            " 12  yr_built       21613 non-null  int64  \n",
            " 13  yr_renovated   21613 non-null  int64  \n",
            " 14  zipcode        21613 non-null  int64  \n",
            " 15  lat            21613 non-null  float64\n",
            " 16  long           21613 non-null  float64\n",
            " 17  sqft_living15  21613 non-null  int64  \n",
            " 18  sqft_lot15     21613 non-null  int64  \n",
            "dtypes: float64(5), int64(14)\n",
            "memory usage: 3.1 MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ThkDxsnGZVRV",
        "outputId": "fef44a1a-6690-4bd7-b618-dd6261194d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21613, 18) (21613, 1)\n"
          ]
        }
      ],
      "source": [
        "y = np.array(df[\"price\"]).reshape(-1,1)\n",
        "X = np.array(df.drop(columns={\"price\"}))\n",
        "print(X.shape, y.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY9JoSCXZVRW"
      },
      "source": [
        "## Create Model\n",
        "\n",
        "Creating a NN model is slightly different from the normal process that we are used to in sklearn. We need to do a little more work to set it up. \n",
        "\n",
        "### Create Model and Add Layers\n",
        "\n",
        "First we need to make a NN model, it comes \"empty\". We will use a sequential model, which is the most simple type but is less configurable (which we don't care about much right now). The limitation of sequential models is that they can only take in one tensor and only output one tensor. The other options here are \"functional\", which allows for the structure of the model to be configured, and \"model subclassing\", which allows you to build almost everything from scratch. \n",
        "\n",
        "#### Layers\n",
        "\n",
        "Next we need to add some layers. We will start simple with only two \"thinking\" layers, and one to do some processing. We can think of the layers roughly like steps of the sklearn pipeline, with data entering at the first layer and predictions flowing out of the final layer. \n",
        "\n",
        "In addition to \"normal\" neural network layers, there are many other types that can do all kinds of other stuff. One example we will use here is the normalization one at the front. This layer does exactly what you'd expect - it normalizes our data so the rest of the network can use it. The normalize layer will also automatically handle the 2D nature of the data that we are used to, so we don't need to worry about that aspect here. Other layers can do everything from regularization to image processing, they are also commonly inhierited for developers to create custom layers targeting specific tasks. We'll use a few of the other ones as we move through things. \n",
        "\n",
        "#### Dense Layers\n",
        "\n",
        "We'll use dense layers here. When adding the layer we need to specify a couple of things. One is the input dimensions - we need to tell the network what the shape of the incomming data is. \n",
        "\n",
        "The other argument is the units, which represents the output dimension. When using these Keras dense layers we don't need to specify each layer's input/output like we did when we made it by hand. We specify both, using units and input_dim, for the first layer that takes in the input; for subsequent layers we can just specify the output and Keras will automatically figure the rest out. \n",
        "\n",
        "Note that there is also an input layer that can be added, we can avoid the need for it by using the input_dim or input_shape as shown below. The two examples there do the same thing, since the input is flat - 18 features. If we are dealing with inputs that do not start out as flat, such as in an image, use the input_shape since you can specify all dimensions; we will see an example of this next time with some images. \n",
        "\n",
        "#### Activation Function\n",
        "\n",
        "For each of our layers we need to define which activation function to use. For now we will use the ReLU function, which is probably the most popular. We'll look at other ones later on. \n",
        "\n",
        "Note that we've left the activation function off of the final layer - we are doing regression so we want that raw value.\n",
        "\n",
        "#### Summary\n",
        "\n",
        "After we've constructed the model, the summary command give us, well, a summary. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIquYAGtZVRX"
      },
      "source": [
        "We are dealing with a bunch of numerical inputs here, so we can add a normalization layer at the front end. Like with sklearn, we want to fit the normalization to the training data only. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zWPhKP3wZVRX",
        "outputId": "4c6bc4d8-4a5f-4da7-edd8-7792b597b931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_1 (Normalizat  (None, 18)               37        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 18)                342       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 19        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 398\n",
            "Trainable params: 361\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(18, input_shape=(18,), activation='relu'))\n",
        "#model.add(Dense(18, input_dim=18, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6AH5AydZVRY"
      },
      "source": [
        "#### Compile Model\n",
        "\n",
        "Once a model is created we need to compile it. The complie step basically builds the layers we specified above and the loss and optimization parameters below together into a usable model object. When compiling the model we are providing it with the things it needs to calculate error:\n",
        "\n",
        "<ul>\n",
        "<li> Loss - we can provide a loss function that we'd like to use. \n",
        "<li> Optimizer - the optimizer is the algorithm that the model will use to perform the gradient descent to find the lowest error. Adam is a very common choice.\n",
        "<li> Learning rate - the learning rate is provided as a parameter of the optimizer. \n",
        "</ul>\n",
        "\n",
        "##### Optimizing Adam\n",
        "\n",
        "The optimizer is the algorithm used to perform the gradient descent and minimize error. For the most part this isn't something we need to be concerned about. The choice of optimizer is much more important if dealing with very large datasets because different optimizers have different levels of efficiency. For our purposes, we can use Adam and be pretty happy. Adam stands for Adaptive Moment Estimation which means basically that it will adjust itself depending on current gradients. It tends to be efficient both in time and memory, so it is very commonly used. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fEya_QckZVRY"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9TUG422ZVRZ"
      },
      "source": [
        "### Fit the Model\n",
        "\n",
        "The fit command does the same thing that we are used to, it trains the model, however there are some differences. The main difference is that batch_size is almost always set in neural networks, while the sklearn models just take all the data at once. \n",
        "\n",
        "What's a batch? Batches are just subsets of the data, so if the batch size is 100 the algorithm will grab 100 rows at a time before making an update to the weights and bias. There are a few reasons this exists:\n",
        "\n",
        "<ul>\n",
        "<li> Memory constraints - it is common with neural networks to deal with datasets that are extremely large. Processing data that can't fit entirely in RAM is very slow (the computer must swap data from the hard drive to RAM as it is needed) compared to data that is in RAM. Cutting the batch size can avoid this issue. \n",
        "<li> Speed - the math involved in the back propagation can sometimes be very computationally intensive. \n",
        "<li> Accuracy - batch size can have an impact on accuracy, though that impact is not very predictable. For the most part finding an optimal batch size will need to be grid-searched. \n",
        "</ul>\n",
        "\n",
        "The fit command also has the epoch paramater, which instructs on how many times to work through ALL of the data. We want to ensure we have enough epochs to find the optimal solution. \n",
        "\n",
        "#### Plot the Loss\n",
        "\n",
        "One very common visualization we see with neural networks is a plot of both training and validation loss vs number of epochs. Generally we'll see the training loss drop - first sharply as the model initially fits itself, then more slowly as it becomes more fitted. The validation loss will usually somewhat mirror the training loss, except it will often reach a minimum at some point before again increasing. This minimum point is our best model, when the validation loss starts increasing again, that is a sign that the model has become overfitted - customized to the training data, but less and less generalizable to new data. \n",
        "\n",
        "Set the verbosity to 1 in the fit to get a full list of the loss for each epoch to pinpoint the exact \"ideal\" number of epochs. We'll look more at this in a minute. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bDUXMj_pZVRZ",
        "outputId": "6facd69a-6643-4845-da6f-94199649ac5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 539372.5000 - val_loss: 551636.4375\n",
            "Epoch 2/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 538971.3750 - val_loss: 550957.2500\n",
            "Epoch 3/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 538060.8125 - val_loss: 549769.3125\n",
            "Epoch 4/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 536672.5000 - val_loss: 548106.2500\n",
            "Epoch 5/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 534833.3125 - val_loss: 545990.0625\n",
            "Epoch 6/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 532562.1250 - val_loss: 543450.5000\n",
            "Epoch 7/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 529886.5625 - val_loss: 540491.1875\n",
            "Epoch 8/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 526821.7500 - val_loss: 537150.1250\n",
            "Epoch 9/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 523384.7812 - val_loss: 533438.3125\n",
            "Epoch 10/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 519591.4375 - val_loss: 529362.8750\n",
            "Epoch 11/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 515455.8438 - val_loss: 524945.1250\n",
            "Epoch 12/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 510992.1250 - val_loss: 520189.5000\n",
            "Epoch 13/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 506212.0312 - val_loss: 515119.2188\n",
            "Epoch 14/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 501128.1562 - val_loss: 509745.4375\n",
            "Epoch 15/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 495767.5312 - val_loss: 504089.7812\n",
            "Epoch 16/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 490145.8125 - val_loss: 498171.4688\n",
            "Epoch 17/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 484273.2188 - val_loss: 492005.7812\n",
            "Epoch 18/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 478163.5312 - val_loss: 485610.1250\n",
            "Epoch 19/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 471837.5938 - val_loss: 479003.2812\n",
            "Epoch 20/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 465330.8125 - val_loss: 472210.7812\n",
            "Epoch 21/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 458655.8438 - val_loss: 465251.3438\n",
            "Epoch 22/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 451843.2500 - val_loss: 458171.2500\n",
            "Epoch 23/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 444914.8438 - val_loss: 451028.8125\n",
            "Epoch 24/1000\n",
            "130/130 [==============================] - 0s 1ms/step - loss: 437968.2500 - val_loss: 443865.4688\n",
            "Epoch 25/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 431061.4062 - val_loss: 436719.0312\n",
            "Epoch 26/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 424226.4062 - val_loss: 429563.0000\n",
            "Epoch 27/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 417469.0938 - val_loss: 422503.9688\n",
            "Epoch 28/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 410793.7500 - val_loss: 415522.4375\n",
            "Epoch 29/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 404192.4062 - val_loss: 408622.4062\n",
            "Epoch 30/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 397686.2500 - val_loss: 401839.4688\n",
            "Epoch 31/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 391267.7500 - val_loss: 395132.1250\n",
            "Epoch 32/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 384971.8438 - val_loss: 388499.4688\n",
            "Epoch 33/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 378783.2500 - val_loss: 381953.9062\n",
            "Epoch 34/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 372678.4062 - val_loss: 375522.6250\n",
            "Epoch 35/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 366674.7188 - val_loss: 369190.8750\n",
            "Epoch 36/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 360800.7500 - val_loss: 362947.0938\n",
            "Epoch 37/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 355021.1250 - val_loss: 356811.0625\n",
            "Epoch 38/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 349351.7812 - val_loss: 350834.4688\n",
            "Epoch 39/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 343732.9062 - val_loss: 344883.5938\n",
            "Epoch 40/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 338171.7812 - val_loss: 339033.7812\n",
            "Epoch 41/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 332674.5312 - val_loss: 333276.7812\n",
            "Epoch 42/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 327232.6250 - val_loss: 327602.2500\n",
            "Epoch 43/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 321881.2500 - val_loss: 322031.0312\n",
            "Epoch 44/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 316605.8750 - val_loss: 316654.5938\n",
            "Epoch 45/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 311410.6562 - val_loss: 311317.9375\n",
            "Epoch 46/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 306300.3438 - val_loss: 306109.0625\n",
            "Epoch 47/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 301297.2188 - val_loss: 301002.0312\n",
            "Epoch 48/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 296469.2500 - val_loss: 296039.1562\n",
            "Epoch 49/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 291759.2188 - val_loss: 291209.3750\n",
            "Epoch 50/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 287213.7500 - val_loss: 286604.5000\n",
            "Epoch 51/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 282795.3750 - val_loss: 282113.4688\n",
            "Epoch 52/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 278534.8750 - val_loss: 277831.9062\n",
            "Epoch 53/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 274410.5625 - val_loss: 273674.5312\n",
            "Epoch 54/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 270376.1562 - val_loss: 269611.7188\n",
            "Epoch 55/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 266445.9062 - val_loss: 265590.0938\n",
            "Epoch 56/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 262643.4688 - val_loss: 261738.4375\n",
            "Epoch 57/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 258924.5781 - val_loss: 257965.3438\n",
            "Epoch 58/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 255323.4062 - val_loss: 254322.2969\n",
            "Epoch 59/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 251792.5781 - val_loss: 250786.0000\n",
            "Epoch 60/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 248322.0000 - val_loss: 247366.1719\n",
            "Epoch 61/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 244937.5938 - val_loss: 244009.3750\n",
            "Epoch 62/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 241626.4375 - val_loss: 240740.6250\n",
            "Epoch 63/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 238442.7656 - val_loss: 237618.9844\n",
            "Epoch 64/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 235361.3906 - val_loss: 234656.5000\n",
            "Epoch 65/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 232418.1094 - val_loss: 231851.5156\n",
            "Epoch 66/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 229612.8750 - val_loss: 229188.7500\n",
            "Epoch 67/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 226942.0156 - val_loss: 226676.7812\n",
            "Epoch 68/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 224413.9844 - val_loss: 224324.2656\n",
            "Epoch 69/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 221995.9219 - val_loss: 222077.7656\n",
            "Epoch 70/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 219695.1719 - val_loss: 219959.3594\n",
            "Epoch 71/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 217479.1562 - val_loss: 217910.1719\n",
            "Epoch 72/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 215348.2656 - val_loss: 215935.9375\n",
            "Epoch 73/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 213270.0781 - val_loss: 213996.5625\n",
            "Epoch 74/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 211259.5469 - val_loss: 212117.9531\n",
            "Epoch 75/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 209306.9219 - val_loss: 210289.4375\n",
            "Epoch 76/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 207406.6094 - val_loss: 208474.8438\n",
            "Epoch 77/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 205552.7969 - val_loss: 206705.9688\n",
            "Epoch 78/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 203743.4688 - val_loss: 204984.2344\n",
            "Epoch 79/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 201966.6875 - val_loss: 203287.3750\n",
            "Epoch 80/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 200227.7188 - val_loss: 201621.2188\n",
            "Epoch 81/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 198510.3594 - val_loss: 199945.2188\n",
            "Epoch 82/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 196836.3281 - val_loss: 198349.4062\n",
            "Epoch 83/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 195223.1094 - val_loss: 196794.8125\n",
            "Epoch 84/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 193624.3906 - val_loss: 195266.9219\n",
            "Epoch 85/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 192052.7969 - val_loss: 193779.8750\n",
            "Epoch 86/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 190522.2188 - val_loss: 192306.6875\n",
            "Epoch 87/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 189048.3281 - val_loss: 190858.9375\n",
            "Epoch 88/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 187597.9375 - val_loss: 189455.0156\n",
            "Epoch 89/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 186173.9844 - val_loss: 188040.0938\n",
            "Epoch 90/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 184789.5312 - val_loss: 186651.7188\n",
            "Epoch 91/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 183457.9844 - val_loss: 185260.2656\n",
            "Epoch 92/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 182144.4375 - val_loss: 183911.7188\n",
            "Epoch 93/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 180862.3281 - val_loss: 182589.5312\n",
            "Epoch 94/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 179602.9219 - val_loss: 181263.4531\n",
            "Epoch 95/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 178345.8438 - val_loss: 179969.7969\n",
            "Epoch 96/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 177103.1562 - val_loss: 178686.8750\n",
            "Epoch 97/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 175869.5000 - val_loss: 177404.3125\n",
            "Epoch 98/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 174654.6406 - val_loss: 176115.1250\n",
            "Epoch 99/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 173450.5781 - val_loss: 174881.2500\n",
            "Epoch 100/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 172276.8281 - val_loss: 173656.5938\n",
            "Epoch 101/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 171114.3750 - val_loss: 172462.9688\n",
            "Epoch 102/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 169963.6094 - val_loss: 171299.9531\n",
            "Epoch 103/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 168827.9375 - val_loss: 170136.2656\n",
            "Epoch 104/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 167704.5625 - val_loss: 169005.6562\n",
            "Epoch 105/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 166593.3906 - val_loss: 167880.7656\n",
            "Epoch 106/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 165482.3125 - val_loss: 166749.6094\n",
            "Epoch 107/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 164386.1875 - val_loss: 165637.4219\n",
            "Epoch 108/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 163303.5469 - val_loss: 164534.0469\n",
            "Epoch 109/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 162223.4219 - val_loss: 163459.3438\n",
            "Epoch 110/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 161145.0156 - val_loss: 162382.4531\n",
            "Epoch 111/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 160073.8594 - val_loss: 161338.5938\n",
            "Epoch 112/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 159016.3906 - val_loss: 160309.3438\n",
            "Epoch 113/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 157972.5469 - val_loss: 159283.9062\n",
            "Epoch 114/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 156949.2656 - val_loss: 158289.7344\n",
            "Epoch 115/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 155947.5156 - val_loss: 157314.6719\n",
            "Epoch 116/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 154979.2812 - val_loss: 156346.8281\n",
            "Epoch 117/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 154039.0000 - val_loss: 155446.6562\n",
            "Epoch 118/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 153124.4219 - val_loss: 154569.8438\n",
            "Epoch 119/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 152244.1250 - val_loss: 153711.7656\n",
            "Epoch 120/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 151377.2344 - val_loss: 152879.0781\n",
            "Epoch 121/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 150542.6719 - val_loss: 152092.4219\n",
            "Epoch 122/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 149736.6250 - val_loss: 151318.2812\n",
            "Epoch 123/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 148942.4531 - val_loss: 150572.5000\n",
            "Epoch 124/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 148161.5625 - val_loss: 149844.0156\n",
            "Epoch 125/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 147401.5625 - val_loss: 149133.0312\n",
            "Epoch 126/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 146670.4219 - val_loss: 148434.2656\n",
            "Epoch 127/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 145949.9062 - val_loss: 147750.7656\n",
            "Epoch 128/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 145250.1719 - val_loss: 147087.8750\n",
            "Epoch 129/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 144568.2188 - val_loss: 146449.9062\n",
            "Epoch 130/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 143902.8281 - val_loss: 145847.8594\n",
            "Epoch 131/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 143256.6250 - val_loss: 145234.4844\n",
            "Epoch 132/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 142630.9375 - val_loss: 144660.4219\n",
            "Epoch 133/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 142018.9688 - val_loss: 144098.1875\n",
            "Epoch 134/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 141431.2969 - val_loss: 143558.9844\n",
            "Epoch 135/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 140862.6562 - val_loss: 143037.0000\n",
            "Epoch 136/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 140310.9375 - val_loss: 142517.0781\n",
            "Epoch 137/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 139767.3750 - val_loss: 142018.0625\n",
            "Epoch 138/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 139242.2031 - val_loss: 141541.3281\n",
            "Epoch 139/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 138727.1250 - val_loss: 141061.2656\n",
            "Epoch 140/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 138224.6250 - val_loss: 140595.5000\n",
            "Epoch 141/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 137734.1875 - val_loss: 140150.3125\n",
            "Epoch 142/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 137259.1719 - val_loss: 139712.2500\n",
            "Epoch 143/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 136789.7656 - val_loss: 139272.0000\n",
            "Epoch 144/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 136336.4844 - val_loss: 138856.7188\n",
            "Epoch 145/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 135894.5781 - val_loss: 138424.7812\n",
            "Epoch 146/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 135453.8281 - val_loss: 138032.4375\n",
            "Epoch 147/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 135026.0625 - val_loss: 137631.6562\n",
            "Epoch 148/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 134608.6250 - val_loss: 137252.5781\n",
            "Epoch 149/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 134203.4375 - val_loss: 136881.4844\n",
            "Epoch 150/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 133808.2031 - val_loss: 136520.7969\n",
            "Epoch 151/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 133420.5312 - val_loss: 136164.1094\n",
            "Epoch 152/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 133035.7656 - val_loss: 135817.4375\n",
            "Epoch 153/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 132664.4062 - val_loss: 135481.8750\n",
            "Epoch 154/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 132299.0625 - val_loss: 135139.8125\n",
            "Epoch 155/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 131940.1562 - val_loss: 134809.6250\n",
            "Epoch 156/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 131584.2500 - val_loss: 134480.6250\n",
            "Epoch 157/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 131234.7344 - val_loss: 134156.6406\n",
            "Epoch 158/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 130886.4141 - val_loss: 133841.8125\n",
            "Epoch 159/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 130542.5312 - val_loss: 133534.3594\n",
            "Epoch 160/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 130202.8906 - val_loss: 133214.3594\n",
            "Epoch 161/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 129866.0703 - val_loss: 132910.5156\n",
            "Epoch 162/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 129536.5547 - val_loss: 132607.7500\n",
            "Epoch 163/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 129209.7656 - val_loss: 132317.6250\n",
            "Epoch 164/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 128895.3984 - val_loss: 132022.1562\n",
            "Epoch 165/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 128582.0391 - val_loss: 131742.7500\n",
            "Epoch 166/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 128276.2188 - val_loss: 131458.8594\n",
            "Epoch 167/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 127973.9141 - val_loss: 131189.3594\n",
            "Epoch 168/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 127675.6328 - val_loss: 130922.4219\n",
            "Epoch 169/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 127383.7266 - val_loss: 130652.4375\n",
            "Epoch 170/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 127102.8438 - val_loss: 130395.5391\n",
            "Epoch 171/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 126815.2500 - val_loss: 130144.0469\n",
            "Epoch 172/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 126537.3516 - val_loss: 129896.7031\n",
            "Epoch 173/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 126255.5859 - val_loss: 129644.1406\n",
            "Epoch 174/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 125981.9922 - val_loss: 129398.3359\n",
            "Epoch 175/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 125714.3516 - val_loss: 129153.7109\n",
            "Epoch 176/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 125443.9297 - val_loss: 128911.1406\n",
            "Epoch 177/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 125181.1562 - val_loss: 128662.7031\n",
            "Epoch 178/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 124920.3672 - val_loss: 128430.1172\n",
            "Epoch 179/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 124662.1797 - val_loss: 128191.2578\n",
            "Epoch 180/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 124406.2109 - val_loss: 127957.7031\n",
            "Epoch 181/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 124150.3125 - val_loss: 127729.1562\n",
            "Epoch 182/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 123901.3438 - val_loss: 127508.0938\n",
            "Epoch 183/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 123657.2969 - val_loss: 127292.2266\n",
            "Epoch 184/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 123422.4141 - val_loss: 127078.9609\n",
            "Epoch 185/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 123184.7578 - val_loss: 126864.6328\n",
            "Epoch 186/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 122952.0391 - val_loss: 126653.1172\n",
            "Epoch 187/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 122724.7734 - val_loss: 126437.1250\n",
            "Epoch 188/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 122503.5625 - val_loss: 126243.2969\n",
            "Epoch 189/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 122281.8125 - val_loss: 126032.5938\n",
            "Epoch 190/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 122065.6875 - val_loss: 125825.6953\n",
            "Epoch 191/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 121852.5469 - val_loss: 125626.9453\n",
            "Epoch 192/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 121643.7422 - val_loss: 125419.9297\n",
            "Epoch 193/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 121436.1172 - val_loss: 125229.1562\n",
            "Epoch 194/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 121228.4844 - val_loss: 125034.4531\n",
            "Epoch 195/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 121030.7344 - val_loss: 124845.3359\n",
            "Epoch 196/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 120825.0781 - val_loss: 124653.6016\n",
            "Epoch 197/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 120625.7344 - val_loss: 124465.9531\n",
            "Epoch 198/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 120426.8594 - val_loss: 124287.5078\n",
            "Epoch 199/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 120235.3047 - val_loss: 124097.8594\n",
            "Epoch 200/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 120040.7422 - val_loss: 123928.2734\n",
            "Epoch 201/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 119854.4531 - val_loss: 123753.8828\n",
            "Epoch 202/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 119664.6719 - val_loss: 123585.2969\n",
            "Epoch 203/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 119480.6094 - val_loss: 123410.1328\n",
            "Epoch 204/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 119291.0156 - val_loss: 123241.1797\n",
            "Epoch 205/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 119107.8281 - val_loss: 123070.0234\n",
            "Epoch 206/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 118922.1016 - val_loss: 122899.6719\n",
            "Epoch 207/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 118740.8359 - val_loss: 122723.4141\n",
            "Epoch 208/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 118557.7578 - val_loss: 122544.9297\n",
            "Epoch 209/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 118382.3828 - val_loss: 122370.9688\n",
            "Epoch 210/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 118204.7734 - val_loss: 122202.6875\n",
            "Epoch 211/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 118029.6328 - val_loss: 122033.4609\n",
            "Epoch 212/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 117857.3125 - val_loss: 121867.3828\n",
            "Epoch 213/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 117688.7578 - val_loss: 121702.0938\n",
            "Epoch 214/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 117521.0391 - val_loss: 121543.1562\n",
            "Epoch 215/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 117362.5859 - val_loss: 121377.0938\n",
            "Epoch 216/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 117192.8281 - val_loss: 121220.7578\n",
            "Epoch 217/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 117036.6953 - val_loss: 121068.3594\n",
            "Epoch 218/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 116878.8750 - val_loss: 120910.7578\n",
            "Epoch 219/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 116725.6094 - val_loss: 120761.5625\n",
            "Epoch 220/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 116568.4844 - val_loss: 120610.0703\n",
            "Epoch 221/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 116416.0156 - val_loss: 120462.3594\n",
            "Epoch 222/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 116264.9766 - val_loss: 120316.1328\n",
            "Epoch 223/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 116120.3438 - val_loss: 120179.0078\n",
            "Epoch 224/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115977.7891 - val_loss: 120043.9453\n",
            "Epoch 225/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115837.7812 - val_loss: 119912.3203\n",
            "Epoch 226/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 115695.7500 - val_loss: 119782.1172\n",
            "Epoch 227/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115555.9375 - val_loss: 119657.9609\n",
            "Epoch 228/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115421.0234 - val_loss: 119526.3281\n",
            "Epoch 229/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115287.5156 - val_loss: 119395.3438\n",
            "Epoch 230/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 115155.2969 - val_loss: 119267.2344\n",
            "Epoch 231/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 115028.2578 - val_loss: 119147.6094\n",
            "Epoch 232/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114896.5625 - val_loss: 119032.0312\n",
            "Epoch 233/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114770.6328 - val_loss: 118913.9375\n",
            "Epoch 234/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114646.1172 - val_loss: 118796.9141\n",
            "Epoch 235/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114526.0312 - val_loss: 118682.9375\n",
            "Epoch 236/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 114409.5312 - val_loss: 118571.8125\n",
            "Epoch 237/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114297.2500 - val_loss: 118455.9922\n",
            "Epoch 238/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114186.0391 - val_loss: 118353.6094\n",
            "Epoch 239/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 114075.5469 - val_loss: 118249.7812\n",
            "Epoch 240/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113968.7422 - val_loss: 118143.8984\n",
            "Epoch 241/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113866.1641 - val_loss: 118047.5781\n",
            "Epoch 242/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113765.2891 - val_loss: 117946.2969\n",
            "Epoch 243/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113664.5625 - val_loss: 117854.9375\n",
            "Epoch 244/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113568.0703 - val_loss: 117756.5938\n",
            "Epoch 245/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113474.3516 - val_loss: 117669.5938\n",
            "Epoch 246/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 113382.1641 - val_loss: 117573.0938\n",
            "Epoch 247/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113292.0938 - val_loss: 117484.8672\n",
            "Epoch 248/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 113201.8594 - val_loss: 117404.8828\n",
            "Epoch 249/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113114.5469 - val_loss: 117319.9766\n",
            "Epoch 250/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 113032.6875 - val_loss: 117248.4531\n",
            "Epoch 251/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 112954.8047 - val_loss: 117163.1328\n",
            "Epoch 252/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112866.9688 - val_loss: 117084.9141\n",
            "Epoch 253/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112788.9453 - val_loss: 117008.5938\n",
            "Epoch 254/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112708.8438 - val_loss: 116937.8828\n",
            "Epoch 255/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112635.8438 - val_loss: 116853.0625\n",
            "Epoch 256/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112556.1562 - val_loss: 116786.3125\n",
            "Epoch 257/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 112483.8281 - val_loss: 116714.4062\n",
            "Epoch 258/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 112401.8359 - val_loss: 116642.0469\n",
            "Epoch 259/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112327.0703 - val_loss: 116577.1953\n",
            "Epoch 260/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 112250.6797 - val_loss: 116511.3984\n",
            "Epoch 261/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112179.3203 - val_loss: 116449.6016\n",
            "Epoch 262/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112105.2891 - val_loss: 116383.7422\n",
            "Epoch 263/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 112032.0781 - val_loss: 116322.1016\n",
            "Epoch 264/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111960.9922 - val_loss: 116259.2656\n",
            "Epoch 265/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111897.0234 - val_loss: 116198.3750\n",
            "Epoch 266/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111828.5703 - val_loss: 116130.9688\n",
            "Epoch 267/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111761.7500 - val_loss: 116066.5469\n",
            "Epoch 268/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111699.2422 - val_loss: 116006.8594\n",
            "Epoch 269/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111635.0391 - val_loss: 115947.0859\n",
            "Epoch 270/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111574.5234 - val_loss: 115893.0469\n",
            "Epoch 271/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111520.7656 - val_loss: 115837.2578\n",
            "Epoch 272/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 111457.2734 - val_loss: 115774.5625\n",
            "Epoch 273/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111401.6562 - val_loss: 115717.5156\n",
            "Epoch 274/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111352.9062 - val_loss: 115663.9219\n",
            "Epoch 275/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111292.8594 - val_loss: 115606.3594\n",
            "Epoch 276/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 111241.9297 - val_loss: 115557.7188\n",
            "Epoch 277/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 111185.4375 - val_loss: 115506.3984\n",
            "Epoch 278/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 111140.3672 - val_loss: 115450.7500\n",
            "Epoch 279/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111085.6953 - val_loss: 115405.4609\n",
            "Epoch 280/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 111032.9844 - val_loss: 115351.5391\n",
            "Epoch 281/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110983.0391 - val_loss: 115307.1328\n",
            "Epoch 282/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110934.4531 - val_loss: 115260.8672\n",
            "Epoch 283/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110886.7812 - val_loss: 115211.3672\n",
            "Epoch 284/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110837.6172 - val_loss: 115161.5234\n",
            "Epoch 285/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110788.5781 - val_loss: 115114.4062\n",
            "Epoch 286/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110744.5781 - val_loss: 115067.9922\n",
            "Epoch 287/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110694.1875 - val_loss: 115027.1797\n",
            "Epoch 288/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110651.5000 - val_loss: 114977.9219\n",
            "Epoch 289/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110602.2734 - val_loss: 114934.6328\n",
            "Epoch 290/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110556.2578 - val_loss: 114887.5234\n",
            "Epoch 291/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110512.4453 - val_loss: 114840.7656\n",
            "Epoch 292/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110466.8203 - val_loss: 114798.1875\n",
            "Epoch 293/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110426.7656 - val_loss: 114748.9531\n",
            "Epoch 294/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110378.5234 - val_loss: 114701.2578\n",
            "Epoch 295/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110339.3984 - val_loss: 114659.6250\n",
            "Epoch 296/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110295.8438 - val_loss: 114617.8125\n",
            "Epoch 297/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110248.7812 - val_loss: 114577.0078\n",
            "Epoch 298/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110210.2344 - val_loss: 114535.3906\n",
            "Epoch 299/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110168.0547 - val_loss: 114494.2031\n",
            "Epoch 300/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110127.1875 - val_loss: 114451.9688\n",
            "Epoch 301/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110086.2656 - val_loss: 114415.0703\n",
            "Epoch 302/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 110050.3203 - val_loss: 114373.0938\n",
            "Epoch 303/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 110008.8750 - val_loss: 114342.8906\n",
            "Epoch 304/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109974.5859 - val_loss: 114298.5000\n",
            "Epoch 305/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109935.4141 - val_loss: 114257.5078\n",
            "Epoch 306/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109896.6406 - val_loss: 114218.4531\n",
            "Epoch 307/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109856.9297 - val_loss: 114179.0703\n",
            "Epoch 308/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109820.0781 - val_loss: 114140.9531\n",
            "Epoch 309/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109783.6172 - val_loss: 114100.7891\n",
            "Epoch 310/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109744.1094 - val_loss: 114067.6094\n",
            "Epoch 311/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109709.1406 - val_loss: 114029.3750\n",
            "Epoch 312/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109675.1094 - val_loss: 113992.3125\n",
            "Epoch 313/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109638.7266 - val_loss: 113955.3047\n",
            "Epoch 314/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109601.1641 - val_loss: 113917.3750\n",
            "Epoch 315/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109568.9688 - val_loss: 113876.3984\n",
            "Epoch 316/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109533.2656 - val_loss: 113842.9922\n",
            "Epoch 317/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109501.7812 - val_loss: 113806.4141\n",
            "Epoch 318/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109465.5859 - val_loss: 113770.7891\n",
            "Epoch 319/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109430.4922 - val_loss: 113735.8594\n",
            "Epoch 320/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109391.2656 - val_loss: 113696.5547\n",
            "Epoch 321/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109360.3750 - val_loss: 113664.4844\n",
            "Epoch 322/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109331.3438 - val_loss: 113630.5703\n",
            "Epoch 323/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109293.3594 - val_loss: 113598.7969\n",
            "Epoch 324/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109255.9141 - val_loss: 113569.8359\n",
            "Epoch 325/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109222.0312 - val_loss: 113539.1094\n",
            "Epoch 326/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109190.5312 - val_loss: 113508.0234\n",
            "Epoch 327/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109154.1797 - val_loss: 113474.3750\n",
            "Epoch 328/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109121.6406 - val_loss: 113443.7109\n",
            "Epoch 329/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109088.9531 - val_loss: 113410.7109\n",
            "Epoch 330/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 109059.1797 - val_loss: 113380.7500\n",
            "Epoch 331/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 109025.5156 - val_loss: 113344.5938\n",
            "Epoch 332/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108993.9609 - val_loss: 113310.3906\n",
            "Epoch 333/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108959.9531 - val_loss: 113286.1953\n",
            "Epoch 334/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108925.6875 - val_loss: 113256.5938\n",
            "Epoch 335/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108896.8594 - val_loss: 113225.5859\n",
            "Epoch 336/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108864.9609 - val_loss: 113197.8203\n",
            "Epoch 337/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108836.1875 - val_loss: 113173.9531\n",
            "Epoch 338/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108804.0547 - val_loss: 113148.1797\n",
            "Epoch 339/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108778.7578 - val_loss: 113123.1172\n",
            "Epoch 340/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108744.1328 - val_loss: 113092.7578\n",
            "Epoch 341/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108714.9766 - val_loss: 113069.4062\n",
            "Epoch 342/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108690.1641 - val_loss: 113039.9531\n",
            "Epoch 343/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108657.6406 - val_loss: 113022.5156\n",
            "Epoch 344/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108630.2422 - val_loss: 112996.8047\n",
            "Epoch 345/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108599.3281 - val_loss: 112970.4062\n",
            "Epoch 346/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108571.8594 - val_loss: 112946.5000\n",
            "Epoch 347/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108543.3594 - val_loss: 112917.5859\n",
            "Epoch 348/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108513.6172 - val_loss: 112897.5859\n",
            "Epoch 349/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108484.0703 - val_loss: 112863.3594\n",
            "Epoch 350/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108458.7812 - val_loss: 112849.0625\n",
            "Epoch 351/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108433.5547 - val_loss: 112827.1641\n",
            "Epoch 352/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108404.5391 - val_loss: 112802.4375\n",
            "Epoch 353/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108380.7500 - val_loss: 112777.1016\n",
            "Epoch 354/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108354.0781 - val_loss: 112753.2656\n",
            "Epoch 355/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108326.3203 - val_loss: 112728.7734\n",
            "Epoch 356/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108304.8984 - val_loss: 112712.2109\n",
            "Epoch 357/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108275.6484 - val_loss: 112686.5859\n",
            "Epoch 358/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108249.8359 - val_loss: 112658.2734\n",
            "Epoch 359/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108228.5000 - val_loss: 112637.3125\n",
            "Epoch 360/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108195.6641 - val_loss: 112617.3906\n",
            "Epoch 361/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108168.8672 - val_loss: 112598.4453\n",
            "Epoch 362/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108142.6875 - val_loss: 112566.0234\n",
            "Epoch 363/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 108116.4688 - val_loss: 112546.7344\n",
            "Epoch 364/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108094.4531 - val_loss: 112519.0469\n",
            "Epoch 365/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108071.1875 - val_loss: 112491.2109\n",
            "Epoch 366/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108044.6094 - val_loss: 112476.5625\n",
            "Epoch 367/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 108017.7891 - val_loss: 112451.0000\n",
            "Epoch 368/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107990.7969 - val_loss: 112429.9141\n",
            "Epoch 369/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107970.8984 - val_loss: 112406.2500\n",
            "Epoch 370/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107944.8203 - val_loss: 112384.9844\n",
            "Epoch 371/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107919.7578 - val_loss: 112369.4219\n",
            "Epoch 372/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107893.8516 - val_loss: 112343.1641\n",
            "Epoch 373/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107869.9844 - val_loss: 112322.2500\n",
            "Epoch 374/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107847.5234 - val_loss: 112303.6172\n",
            "Epoch 375/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107820.7188 - val_loss: 112281.0234\n",
            "Epoch 376/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107801.2578 - val_loss: 112259.3750\n",
            "Epoch 377/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107774.2500 - val_loss: 112239.7031\n",
            "Epoch 378/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107756.9297 - val_loss: 112215.3906\n",
            "Epoch 379/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107729.6719 - val_loss: 112196.6328\n",
            "Epoch 380/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107707.9688 - val_loss: 112180.3047\n",
            "Epoch 381/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107682.1953 - val_loss: 112159.9375\n",
            "Epoch 382/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107656.0156 - val_loss: 112143.0859\n",
            "Epoch 383/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107639.7266 - val_loss: 112127.3594\n",
            "Epoch 384/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107612.2656 - val_loss: 112113.1875\n",
            "Epoch 385/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107587.3281 - val_loss: 112089.5859\n",
            "Epoch 386/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107567.4141 - val_loss: 112068.5859\n",
            "Epoch 387/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107545.2422 - val_loss: 112052.6953\n",
            "Epoch 388/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107526.7656 - val_loss: 112029.8125\n",
            "Epoch 389/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107499.3359 - val_loss: 112013.2891\n",
            "Epoch 390/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107477.9688 - val_loss: 111989.1797\n",
            "Epoch 391/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107454.0000 - val_loss: 111964.3203\n",
            "Epoch 392/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107432.3672 - val_loss: 111951.4688\n",
            "Epoch 393/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107412.6797 - val_loss: 111932.4453\n",
            "Epoch 394/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107389.7891 - val_loss: 111913.7656\n",
            "Epoch 395/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107369.2188 - val_loss: 111895.6875\n",
            "Epoch 396/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107346.0312 - val_loss: 111871.7266\n",
            "Epoch 397/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107331.5859 - val_loss: 111850.2344\n",
            "Epoch 398/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107304.0781 - val_loss: 111838.5391\n",
            "Epoch 399/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107285.8438 - val_loss: 111817.3359\n",
            "Epoch 400/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107263.2500 - val_loss: 111801.9297\n",
            "Epoch 401/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107242.6562 - val_loss: 111780.2344\n",
            "Epoch 402/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107221.9062 - val_loss: 111764.7734\n",
            "Epoch 403/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107205.3438 - val_loss: 111740.0938\n",
            "Epoch 404/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107182.4375 - val_loss: 111730.5000\n",
            "Epoch 405/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107162.2734 - val_loss: 111709.8750\n",
            "Epoch 406/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107142.8281 - val_loss: 111689.4062\n",
            "Epoch 407/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107122.1016 - val_loss: 111672.2891\n",
            "Epoch 408/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107100.4609 - val_loss: 111659.6016\n",
            "Epoch 409/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107082.6484 - val_loss: 111641.6328\n",
            "Epoch 410/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 107064.2031 - val_loss: 111627.9531\n",
            "Epoch 411/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107044.7422 - val_loss: 111601.4844\n",
            "Epoch 412/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107024.8047 - val_loss: 111590.5859\n",
            "Epoch 413/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 107008.9688 - val_loss: 111569.0547\n",
            "Epoch 414/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106988.3203 - val_loss: 111555.0234\n",
            "Epoch 415/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106968.9688 - val_loss: 111535.8594\n",
            "Epoch 416/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106950.0938 - val_loss: 111514.5391\n",
            "Epoch 417/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106933.0078 - val_loss: 111493.0312\n",
            "Epoch 418/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106914.8047 - val_loss: 111480.8672\n",
            "Epoch 419/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106899.5547 - val_loss: 111458.5547\n",
            "Epoch 420/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106878.8359 - val_loss: 111440.0312\n",
            "Epoch 421/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106861.7344 - val_loss: 111421.1562\n",
            "Epoch 422/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106840.0391 - val_loss: 111404.5234\n",
            "Epoch 423/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106823.8828 - val_loss: 111391.4766\n",
            "Epoch 424/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106807.6562 - val_loss: 111369.2422\n",
            "Epoch 425/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106786.0547 - val_loss: 111350.5234\n",
            "Epoch 426/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106771.8984 - val_loss: 111337.0312\n",
            "Epoch 427/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106755.0859 - val_loss: 111314.0625\n",
            "Epoch 428/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106738.2188 - val_loss: 111298.6094\n",
            "Epoch 429/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106716.9297 - val_loss: 111273.3594\n",
            "Epoch 430/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106699.4062 - val_loss: 111257.9531\n",
            "Epoch 431/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106683.5938 - val_loss: 111240.1797\n",
            "Epoch 432/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106665.7188 - val_loss: 111224.1562\n",
            "Epoch 433/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106647.1250 - val_loss: 111205.5703\n",
            "Epoch 434/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106633.7500 - val_loss: 111187.6094\n",
            "Epoch 435/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106618.0078 - val_loss: 111165.2500\n",
            "Epoch 436/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106598.8984 - val_loss: 111149.7969\n",
            "Epoch 437/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106582.7266 - val_loss: 111137.7656\n",
            "Epoch 438/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106561.5781 - val_loss: 111117.3125\n",
            "Epoch 439/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106545.9766 - val_loss: 111098.2656\n",
            "Epoch 440/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106528.3359 - val_loss: 111079.7578\n",
            "Epoch 441/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106514.4375 - val_loss: 111056.4922\n",
            "Epoch 442/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106497.1875 - val_loss: 111040.4844\n",
            "Epoch 443/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106479.0781 - val_loss: 111023.4688\n",
            "Epoch 444/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106460.1250 - val_loss: 111011.1328\n",
            "Epoch 445/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106445.7266 - val_loss: 110989.1250\n",
            "Epoch 446/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106428.4922 - val_loss: 110972.0781\n",
            "Epoch 447/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106414.4766 - val_loss: 110954.2266\n",
            "Epoch 448/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106397.9297 - val_loss: 110933.9219\n",
            "Epoch 449/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106383.8359 - val_loss: 110922.0391\n",
            "Epoch 450/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106366.9531 - val_loss: 110896.6484\n",
            "Epoch 451/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106349.8594 - val_loss: 110879.6484\n",
            "Epoch 452/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106333.5000 - val_loss: 110864.8359\n",
            "Epoch 453/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106316.4922 - val_loss: 110848.2656\n",
            "Epoch 454/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106302.7969 - val_loss: 110829.0703\n",
            "Epoch 455/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106287.3984 - val_loss: 110811.8516\n",
            "Epoch 456/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106267.7109 - val_loss: 110800.3672\n",
            "Epoch 457/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106252.9844 - val_loss: 110778.5781\n",
            "Epoch 458/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106238.0859 - val_loss: 110768.8594\n",
            "Epoch 459/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106221.9688 - val_loss: 110749.7266\n",
            "Epoch 460/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106209.7344 - val_loss: 110734.6250\n",
            "Epoch 461/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106192.2891 - val_loss: 110715.7656\n",
            "Epoch 462/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106175.2188 - val_loss: 110699.4219\n",
            "Epoch 463/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106157.9609 - val_loss: 110687.7109\n",
            "Epoch 464/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106145.5938 - val_loss: 110664.4453\n",
            "Epoch 465/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106131.2266 - val_loss: 110653.2344\n",
            "Epoch 466/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106111.3125 - val_loss: 110637.3125\n",
            "Epoch 467/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106100.7500 - val_loss: 110617.1172\n",
            "Epoch 468/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106084.6641 - val_loss: 110604.7422\n",
            "Epoch 469/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106064.4297 - val_loss: 110587.3516\n",
            "Epoch 470/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106050.4609 - val_loss: 110579.9062\n",
            "Epoch 471/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 106036.6016 - val_loss: 110564.3594\n",
            "Epoch 472/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106019.1094 - val_loss: 110547.1172\n",
            "Epoch 473/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 106004.8516 - val_loss: 110535.5078\n",
            "Epoch 474/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105992.3672 - val_loss: 110518.3828\n",
            "Epoch 475/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105970.5547 - val_loss: 110502.3203\n",
            "Epoch 476/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105956.0312 - val_loss: 110492.4922\n",
            "Epoch 477/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105946.0156 - val_loss: 110477.9453\n",
            "Epoch 478/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105927.0000 - val_loss: 110457.7812\n",
            "Epoch 479/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105910.8516 - val_loss: 110446.8359\n",
            "Epoch 480/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105897.4688 - val_loss: 110426.3672\n",
            "Epoch 481/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105879.3359 - val_loss: 110414.2734\n",
            "Epoch 482/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105866.8672 - val_loss: 110404.9844\n",
            "Epoch 483/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105855.1094 - val_loss: 110386.0156\n",
            "Epoch 484/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105839.7422 - val_loss: 110373.5391\n",
            "Epoch 485/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105821.6172 - val_loss: 110358.5469\n",
            "Epoch 486/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105808.3516 - val_loss: 110345.5859\n",
            "Epoch 487/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105790.2969 - val_loss: 110330.1641\n",
            "Epoch 488/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105775.2344 - val_loss: 110317.3516\n",
            "Epoch 489/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105765.8828 - val_loss: 110297.2656\n",
            "Epoch 490/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105751.9297 - val_loss: 110284.3984\n",
            "Epoch 491/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105733.0078 - val_loss: 110268.8672\n",
            "Epoch 492/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105719.7500 - val_loss: 110255.6641\n",
            "Epoch 493/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105705.1094 - val_loss: 110240.9766\n",
            "Epoch 494/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105688.8047 - val_loss: 110224.1172\n",
            "Epoch 495/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105672.9297 - val_loss: 110209.7344\n",
            "Epoch 496/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105661.0078 - val_loss: 110192.7266\n",
            "Epoch 497/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105646.3672 - val_loss: 110181.6641\n",
            "Epoch 498/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105631.0859 - val_loss: 110163.7969\n",
            "Epoch 499/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105617.6562 - val_loss: 110147.6719\n",
            "Epoch 500/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105600.9453 - val_loss: 110134.2891\n",
            "Epoch 501/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105588.9219 - val_loss: 110116.0234\n",
            "Epoch 502/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105577.1562 - val_loss: 110101.5156\n",
            "Epoch 503/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105560.1562 - val_loss: 110088.8125\n",
            "Epoch 504/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105542.5703 - val_loss: 110075.1328\n",
            "Epoch 505/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105533.2500 - val_loss: 110059.6094\n",
            "Epoch 506/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105515.5938 - val_loss: 110043.9453\n",
            "Epoch 507/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105505.8281 - val_loss: 110028.6328\n",
            "Epoch 508/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105492.0781 - val_loss: 110019.4141\n",
            "Epoch 509/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105475.8828 - val_loss: 110005.6641\n",
            "Epoch 510/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105460.0938 - val_loss: 109992.2031\n",
            "Epoch 511/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105446.6875 - val_loss: 109973.9688\n",
            "Epoch 512/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105431.8516 - val_loss: 109960.8906\n",
            "Epoch 513/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105418.8594 - val_loss: 109947.2266\n",
            "Epoch 514/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105405.5000 - val_loss: 109933.9766\n",
            "Epoch 515/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105393.0312 - val_loss: 109921.1875\n",
            "Epoch 516/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105376.8984 - val_loss: 109902.5391\n",
            "Epoch 517/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105363.2812 - val_loss: 109888.0234\n",
            "Epoch 518/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105350.9609 - val_loss: 109875.7969\n",
            "Epoch 519/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105335.6719 - val_loss: 109862.1875\n",
            "Epoch 520/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105317.9766 - val_loss: 109850.9453\n",
            "Epoch 521/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105308.5625 - val_loss: 109834.9141\n",
            "Epoch 522/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105294.6250 - val_loss: 109824.1797\n",
            "Epoch 523/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105280.0234 - val_loss: 109814.6797\n",
            "Epoch 524/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105267.9141 - val_loss: 109793.9531\n",
            "Epoch 525/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105249.2734 - val_loss: 109783.0078\n",
            "Epoch 526/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105238.3281 - val_loss: 109769.7734\n",
            "Epoch 527/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105227.8438 - val_loss: 109750.2188\n",
            "Epoch 528/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105213.0469 - val_loss: 109740.1328\n",
            "Epoch 529/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105196.3438 - val_loss: 109731.7812\n",
            "Epoch 530/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105185.2891 - val_loss: 109717.5000\n",
            "Epoch 531/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105172.4609 - val_loss: 109699.6016\n",
            "Epoch 532/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105156.5078 - val_loss: 109687.0703\n",
            "Epoch 533/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105147.3984 - val_loss: 109673.8281\n",
            "Epoch 534/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105129.6562 - val_loss: 109660.9141\n",
            "Epoch 535/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105116.7656 - val_loss: 109649.8125\n",
            "Epoch 536/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105102.2344 - val_loss: 109637.1328\n",
            "Epoch 537/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105090.4688 - val_loss: 109618.6953\n",
            "Epoch 538/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105077.4297 - val_loss: 109606.8984\n",
            "Epoch 539/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105067.2188 - val_loss: 109591.0312\n",
            "Epoch 540/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105052.0312 - val_loss: 109574.1172\n",
            "Epoch 541/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 105034.3594 - val_loss: 109563.0156\n",
            "Epoch 542/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105020.6094 - val_loss: 109550.3125\n",
            "Epoch 543/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 105006.9297 - val_loss: 109533.4141\n",
            "Epoch 544/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104998.5703 - val_loss: 109523.0859\n",
            "Epoch 545/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104982.2578 - val_loss: 109511.6016\n",
            "Epoch 546/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104971.0547 - val_loss: 109495.4141\n",
            "Epoch 547/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104958.9375 - val_loss: 109483.6953\n",
            "Epoch 548/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104946.6328 - val_loss: 109475.8281\n",
            "Epoch 549/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104929.3906 - val_loss: 109460.6172\n",
            "Epoch 550/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104919.4688 - val_loss: 109451.9922\n",
            "Epoch 551/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104907.4297 - val_loss: 109433.5625\n",
            "Epoch 552/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104888.2969 - val_loss: 109422.2344\n",
            "Epoch 553/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104881.6719 - val_loss: 109412.1484\n",
            "Epoch 554/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104865.9766 - val_loss: 109397.1016\n",
            "Epoch 555/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104851.4766 - val_loss: 109381.1562\n",
            "Epoch 556/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104839.9453 - val_loss: 109369.0234\n",
            "Epoch 557/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104826.9531 - val_loss: 109361.7500\n",
            "Epoch 558/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104814.5781 - val_loss: 109345.8672\n",
            "Epoch 559/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104803.7266 - val_loss: 109329.0000\n",
            "Epoch 560/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104789.3594 - val_loss: 109317.1562\n",
            "Epoch 561/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104777.1641 - val_loss: 109308.2109\n",
            "Epoch 562/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104764.9531 - val_loss: 109298.0703\n",
            "Epoch 563/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104750.7266 - val_loss: 109282.9141\n",
            "Epoch 564/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104738.4297 - val_loss: 109269.5391\n",
            "Epoch 565/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104725.2812 - val_loss: 109258.9688\n",
            "Epoch 566/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104713.7734 - val_loss: 109246.9375\n",
            "Epoch 567/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104705.7109 - val_loss: 109234.2969\n",
            "Epoch 568/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104688.2188 - val_loss: 109220.4531\n",
            "Epoch 569/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104677.7891 - val_loss: 109204.6172\n",
            "Epoch 570/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104664.8828 - val_loss: 109195.1406\n",
            "Epoch 571/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104654.6719 - val_loss: 109182.0000\n",
            "Epoch 572/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104643.3203 - val_loss: 109167.0625\n",
            "Epoch 573/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104628.3984 - val_loss: 109155.2734\n",
            "Epoch 574/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104617.5000 - val_loss: 109146.0312\n",
            "Epoch 575/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104604.0938 - val_loss: 109134.3906\n",
            "Epoch 576/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104595.2031 - val_loss: 109122.0156\n",
            "Epoch 577/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104583.0391 - val_loss: 109111.0312\n",
            "Epoch 578/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104567.6016 - val_loss: 109095.5625\n",
            "Epoch 579/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104559.5781 - val_loss: 109080.7578\n",
            "Epoch 580/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104547.7969 - val_loss: 109073.9922\n",
            "Epoch 581/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104531.9375 - val_loss: 109059.5312\n",
            "Epoch 582/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104524.5234 - val_loss: 109044.1953\n",
            "Epoch 583/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104512.0859 - val_loss: 109032.9922\n",
            "Epoch 584/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104497.4375 - val_loss: 109019.0938\n",
            "Epoch 585/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104486.1250 - val_loss: 109007.1406\n",
            "Epoch 586/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104474.1094 - val_loss: 108998.5156\n",
            "Epoch 587/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104461.8594 - val_loss: 108987.0391\n",
            "Epoch 588/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104448.8672 - val_loss: 108972.5469\n",
            "Epoch 589/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104440.9062 - val_loss: 108960.5391\n",
            "Epoch 590/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104431.5234 - val_loss: 108946.1250\n",
            "Epoch 591/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104415.8359 - val_loss: 108934.8125\n",
            "Epoch 592/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104405.6797 - val_loss: 108924.1406\n",
            "Epoch 593/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104399.2734 - val_loss: 108916.6406\n",
            "Epoch 594/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104382.8984 - val_loss: 108897.0859\n",
            "Epoch 595/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104371.3438 - val_loss: 108891.9609\n",
            "Epoch 596/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104358.7812 - val_loss: 108881.9766\n",
            "Epoch 597/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104348.3438 - val_loss: 108869.1719\n",
            "Epoch 598/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104336.8984 - val_loss: 108860.1719\n",
            "Epoch 599/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104326.8203 - val_loss: 108851.5312\n",
            "Epoch 600/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104315.7188 - val_loss: 108837.0547\n",
            "Epoch 601/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104303.4844 - val_loss: 108829.4531\n",
            "Epoch 602/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104294.8359 - val_loss: 108811.4688\n",
            "Epoch 603/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104283.4062 - val_loss: 108804.5469\n",
            "Epoch 604/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104272.4141 - val_loss: 108796.7109\n",
            "Epoch 605/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104260.6016 - val_loss: 108785.4922\n",
            "Epoch 606/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104248.6094 - val_loss: 108774.2109\n",
            "Epoch 607/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104239.4922 - val_loss: 108763.1172\n",
            "Epoch 608/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104229.6172 - val_loss: 108750.0547\n",
            "Epoch 609/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104218.2734 - val_loss: 108745.8047\n",
            "Epoch 610/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104207.1484 - val_loss: 108734.4141\n",
            "Epoch 611/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104198.0391 - val_loss: 108724.8281\n",
            "Epoch 612/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104186.5703 - val_loss: 108719.8594\n",
            "Epoch 613/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104175.3672 - val_loss: 108701.2656\n",
            "Epoch 614/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104161.2812 - val_loss: 108691.3203\n",
            "Epoch 615/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104154.1016 - val_loss: 108684.1172\n",
            "Epoch 616/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104140.9766 - val_loss: 108670.7891\n",
            "Epoch 617/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104130.6406 - val_loss: 108668.6562\n",
            "Epoch 618/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104118.8438 - val_loss: 108658.6094\n",
            "Epoch 619/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104110.3750 - val_loss: 108645.0312\n",
            "Epoch 620/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104099.0312 - val_loss: 108637.5234\n",
            "Epoch 621/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104086.8984 - val_loss: 108621.2656\n",
            "Epoch 622/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104077.4375 - val_loss: 108614.2266\n",
            "Epoch 623/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104069.5547 - val_loss: 108602.2344\n",
            "Epoch 624/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104059.0156 - val_loss: 108590.4375\n",
            "Epoch 625/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104045.0000 - val_loss: 108575.9375\n",
            "Epoch 626/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 104035.9375 - val_loss: 108575.2422\n",
            "Epoch 627/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104024.0938 - val_loss: 108566.6328\n",
            "Epoch 628/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104022.2969 - val_loss: 108554.2656\n",
            "Epoch 629/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104004.9531 - val_loss: 108540.3047\n",
            "Epoch 630/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 104001.7188 - val_loss: 108533.0078\n",
            "Epoch 631/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103983.4688 - val_loss: 108520.8203\n",
            "Epoch 632/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103975.5703 - val_loss: 108510.7734\n",
            "Epoch 633/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103965.6172 - val_loss: 108499.1328\n",
            "Epoch 634/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103956.9922 - val_loss: 108493.7422\n",
            "Epoch 635/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103943.7188 - val_loss: 108480.1641\n",
            "Epoch 636/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103935.8906 - val_loss: 108468.5078\n",
            "Epoch 637/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103923.6406 - val_loss: 108459.9062\n",
            "Epoch 638/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103913.2578 - val_loss: 108442.0703\n",
            "Epoch 639/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103903.3047 - val_loss: 108430.9922\n",
            "Epoch 640/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103894.9688 - val_loss: 108418.5078\n",
            "Epoch 641/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103882.5000 - val_loss: 108416.5000\n",
            "Epoch 642/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103872.9922 - val_loss: 108403.7500\n",
            "Epoch 643/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103865.6406 - val_loss: 108395.7344\n",
            "Epoch 644/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103854.9375 - val_loss: 108382.5469\n",
            "Epoch 645/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103842.2344 - val_loss: 108375.2188\n",
            "Epoch 646/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103835.7656 - val_loss: 108363.2266\n",
            "Epoch 647/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103826.4062 - val_loss: 108350.8359\n",
            "Epoch 648/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103813.6562 - val_loss: 108342.3750\n",
            "Epoch 649/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103804.5156 - val_loss: 108332.0781\n",
            "Epoch 650/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103793.2344 - val_loss: 108318.2344\n",
            "Epoch 651/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103784.3359 - val_loss: 108305.7891\n",
            "Epoch 652/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103775.0234 - val_loss: 108301.4844\n",
            "Epoch 653/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103765.0078 - val_loss: 108293.4219\n",
            "Epoch 654/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103757.9844 - val_loss: 108281.8438\n",
            "Epoch 655/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103746.6875 - val_loss: 108266.9844\n",
            "Epoch 656/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103734.4062 - val_loss: 108253.9375\n",
            "Epoch 657/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103729.0000 - val_loss: 108244.2578\n",
            "Epoch 658/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103716.8281 - val_loss: 108233.0234\n",
            "Epoch 659/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103709.0000 - val_loss: 108227.2109\n",
            "Epoch 660/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103698.9688 - val_loss: 108217.0703\n",
            "Epoch 661/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103686.4141 - val_loss: 108208.2656\n",
            "Epoch 662/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103680.6406 - val_loss: 108202.4609\n",
            "Epoch 663/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103666.6641 - val_loss: 108189.1406\n",
            "Epoch 664/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103658.0859 - val_loss: 108179.6250\n",
            "Epoch 665/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103646.9688 - val_loss: 108169.5078\n",
            "Epoch 666/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103639.7266 - val_loss: 108155.2656\n",
            "Epoch 667/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103632.0469 - val_loss: 108146.1484\n",
            "Epoch 668/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103623.6953 - val_loss: 108129.2422\n",
            "Epoch 669/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103607.5547 - val_loss: 108123.5547\n",
            "Epoch 670/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103597.2656 - val_loss: 108111.9609\n",
            "Epoch 671/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103590.5000 - val_loss: 108099.9844\n",
            "Epoch 672/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103580.6250 - val_loss: 108090.1094\n",
            "Epoch 673/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103573.9375 - val_loss: 108085.0703\n",
            "Epoch 674/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103563.5078 - val_loss: 108075.4922\n",
            "Epoch 675/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103553.3594 - val_loss: 108058.2266\n",
            "Epoch 676/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103545.1406 - val_loss: 108053.1562\n",
            "Epoch 677/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103534.7188 - val_loss: 108045.2031\n",
            "Epoch 678/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103523.9531 - val_loss: 108033.3438\n",
            "Epoch 679/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103513.4062 - val_loss: 108016.0781\n",
            "Epoch 680/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103504.5703 - val_loss: 108005.1719\n",
            "Epoch 681/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103495.9609 - val_loss: 107997.9531\n",
            "Epoch 682/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103484.1641 - val_loss: 107985.1875\n",
            "Epoch 683/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103474.3906 - val_loss: 107982.0781\n",
            "Epoch 684/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103466.3438 - val_loss: 107963.8438\n",
            "Epoch 685/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103456.6797 - val_loss: 107957.0312\n",
            "Epoch 686/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103447.8672 - val_loss: 107945.0703\n",
            "Epoch 687/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103438.7578 - val_loss: 107934.0625\n",
            "Epoch 688/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103433.4688 - val_loss: 107918.8359\n",
            "Epoch 689/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103420.6719 - val_loss: 107909.4062\n",
            "Epoch 690/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103409.9297 - val_loss: 107901.7188\n",
            "Epoch 691/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103398.6250 - val_loss: 107890.0156\n",
            "Epoch 692/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103389.9844 - val_loss: 107878.1875\n",
            "Epoch 693/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103381.7734 - val_loss: 107866.3672\n",
            "Epoch 694/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103373.2109 - val_loss: 107859.9062\n",
            "Epoch 695/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103360.9453 - val_loss: 107852.7812\n",
            "Epoch 696/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103351.5391 - val_loss: 107840.0312\n",
            "Epoch 697/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103344.9531 - val_loss: 107828.8047\n",
            "Epoch 698/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103334.7422 - val_loss: 107819.3125\n",
            "Epoch 699/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103324.7344 - val_loss: 107810.1016\n",
            "Epoch 700/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103316.2266 - val_loss: 107792.6016\n",
            "Epoch 701/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103305.9766 - val_loss: 107786.7109\n",
            "Epoch 702/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103294.8281 - val_loss: 107776.3750\n",
            "Epoch 703/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103288.6562 - val_loss: 107766.2500\n",
            "Epoch 704/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103280.5547 - val_loss: 107757.1406\n",
            "Epoch 705/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103268.1953 - val_loss: 107742.2500\n",
            "Epoch 706/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103253.4219 - val_loss: 107725.6641\n",
            "Epoch 707/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103246.5234 - val_loss: 107725.6250\n",
            "Epoch 708/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103238.1172 - val_loss: 107709.7109\n",
            "Epoch 709/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103231.9844 - val_loss: 107699.9297\n",
            "Epoch 710/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103218.0234 - val_loss: 107689.3281\n",
            "Epoch 711/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103205.1562 - val_loss: 107675.6016\n",
            "Epoch 712/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103201.2734 - val_loss: 107670.6406\n",
            "Epoch 713/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103190.1875 - val_loss: 107657.6562\n",
            "Epoch 714/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103178.6797 - val_loss: 107645.7422\n",
            "Epoch 715/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103168.3828 - val_loss: 107630.9219\n",
            "Epoch 716/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103159.9531 - val_loss: 107614.2266\n",
            "Epoch 717/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103150.5391 - val_loss: 107609.7734\n",
            "Epoch 718/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103140.5859 - val_loss: 107596.5625\n",
            "Epoch 719/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103131.2109 - val_loss: 107581.5625\n",
            "Epoch 720/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103125.0234 - val_loss: 107566.3984\n",
            "Epoch 721/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103117.1250 - val_loss: 107568.4141\n",
            "Epoch 722/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103101.3203 - val_loss: 107560.2812\n",
            "Epoch 723/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103098.1875 - val_loss: 107544.3047\n",
            "Epoch 724/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103083.0391 - val_loss: 107536.6328\n",
            "Epoch 725/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103073.5391 - val_loss: 107526.8672\n",
            "Epoch 726/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103063.7812 - val_loss: 107514.5391\n",
            "Epoch 727/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103058.9609 - val_loss: 107503.5859\n",
            "Epoch 728/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 103049.4141 - val_loss: 107495.4219\n",
            "Epoch 729/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103040.2734 - val_loss: 107481.0234\n",
            "Epoch 730/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103028.0547 - val_loss: 107472.8516\n",
            "Epoch 731/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103021.6562 - val_loss: 107458.3984\n",
            "Epoch 732/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103011.3516 - val_loss: 107445.9844\n",
            "Epoch 733/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 103000.5938 - val_loss: 107432.7969\n",
            "Epoch 734/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102993.6250 - val_loss: 107418.9531\n",
            "Epoch 735/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102981.4844 - val_loss: 107415.2734\n",
            "Epoch 736/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102973.5547 - val_loss: 107405.9453\n",
            "Epoch 737/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102962.3359 - val_loss: 107392.9844\n",
            "Epoch 738/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102953.8281 - val_loss: 107387.1328\n",
            "Epoch 739/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102945.7656 - val_loss: 107375.1016\n",
            "Epoch 740/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102935.9688 - val_loss: 107364.0781\n",
            "Epoch 741/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102930.9141 - val_loss: 107344.9844\n",
            "Epoch 742/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102917.6797 - val_loss: 107340.1016\n",
            "Epoch 743/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102909.7031 - val_loss: 107336.8750\n",
            "Epoch 744/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102901.0859 - val_loss: 107322.7500\n",
            "Epoch 745/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102891.9453 - val_loss: 107309.5703\n",
            "Epoch 746/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102884.5703 - val_loss: 107310.9453\n",
            "Epoch 747/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102879.8594 - val_loss: 107298.3906\n",
            "Epoch 748/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102865.1328 - val_loss: 107278.8438\n",
            "Epoch 749/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102858.3203 - val_loss: 107269.7578\n",
            "Epoch 750/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102848.1953 - val_loss: 107256.1641\n",
            "Epoch 751/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102838.4922 - val_loss: 107247.7734\n",
            "Epoch 752/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102832.2734 - val_loss: 107243.2422\n",
            "Epoch 753/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102822.2969 - val_loss: 107234.8750\n",
            "Epoch 754/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102815.9609 - val_loss: 107227.6484\n",
            "Epoch 755/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102808.1719 - val_loss: 107220.7188\n",
            "Epoch 756/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102798.1562 - val_loss: 107207.7969\n",
            "Epoch 757/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102787.9844 - val_loss: 107191.2734\n",
            "Epoch 758/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102782.5469 - val_loss: 107186.2734\n",
            "Epoch 759/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102774.8203 - val_loss: 107175.5312\n",
            "Epoch 760/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102767.6172 - val_loss: 107162.5547\n",
            "Epoch 761/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102756.0312 - val_loss: 107156.3438\n",
            "Epoch 762/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102753.9453 - val_loss: 107146.8906\n",
            "Epoch 763/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102743.5469 - val_loss: 107146.2266\n",
            "Epoch 764/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102735.3438 - val_loss: 107131.5781\n",
            "Epoch 765/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102725.0312 - val_loss: 107120.4219\n",
            "Epoch 766/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102715.8750 - val_loss: 107108.7734\n",
            "Epoch 767/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102710.2734 - val_loss: 107100.4766\n",
            "Epoch 768/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102700.3438 - val_loss: 107097.5781\n",
            "Epoch 769/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102693.6484 - val_loss: 107092.3828\n",
            "Epoch 770/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102689.0312 - val_loss: 107071.6562\n",
            "Epoch 771/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102677.1172 - val_loss: 107069.4922\n",
            "Epoch 772/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102668.9609 - val_loss: 107052.2109\n",
            "Epoch 773/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102664.1016 - val_loss: 107051.1719\n",
            "Epoch 774/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102654.4375 - val_loss: 107035.3906\n",
            "Epoch 775/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102645.4219 - val_loss: 107021.8516\n",
            "Epoch 776/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102638.9844 - val_loss: 107019.6641\n",
            "Epoch 777/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102632.0234 - val_loss: 107008.1016\n",
            "Epoch 778/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102621.4609 - val_loss: 107009.0000\n",
            "Epoch 779/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102619.5938 - val_loss: 107004.5938\n",
            "Epoch 780/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102604.9688 - val_loss: 106992.7266\n",
            "Epoch 781/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102600.9297 - val_loss: 106980.4453\n",
            "Epoch 782/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102594.8984 - val_loss: 106975.2891\n",
            "Epoch 783/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102583.3906 - val_loss: 106955.8750\n",
            "Epoch 784/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102578.8984 - val_loss: 106951.2188\n",
            "Epoch 785/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102567.4844 - val_loss: 106938.9453\n",
            "Epoch 786/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102561.4922 - val_loss: 106940.8828\n",
            "Epoch 787/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102552.4922 - val_loss: 106931.2891\n",
            "Epoch 788/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102545.5781 - val_loss: 106928.2891\n",
            "Epoch 789/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102536.1719 - val_loss: 106906.8203\n",
            "Epoch 790/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102526.2344 - val_loss: 106902.3281\n",
            "Epoch 791/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102522.3672 - val_loss: 106891.6797\n",
            "Epoch 792/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102518.5703 - val_loss: 106883.5625\n",
            "Epoch 793/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102506.7266 - val_loss: 106878.5703\n",
            "Epoch 794/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102498.3672 - val_loss: 106870.6953\n",
            "Epoch 795/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102490.5000 - val_loss: 106855.9141\n",
            "Epoch 796/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102482.9375 - val_loss: 106848.3516\n",
            "Epoch 797/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102475.9453 - val_loss: 106833.2500\n",
            "Epoch 798/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102470.3359 - val_loss: 106830.1016\n",
            "Epoch 799/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102461.9219 - val_loss: 106821.5938\n",
            "Epoch 800/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102451.7656 - val_loss: 106809.7109\n",
            "Epoch 801/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102447.8359 - val_loss: 106803.8984\n",
            "Epoch 802/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102437.6016 - val_loss: 106791.9141\n",
            "Epoch 803/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102432.0547 - val_loss: 106786.2500\n",
            "Epoch 804/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102424.8359 - val_loss: 106774.4375\n",
            "Epoch 805/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102417.5703 - val_loss: 106768.2109\n",
            "Epoch 806/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102411.7344 - val_loss: 106762.3047\n",
            "Epoch 807/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102405.7188 - val_loss: 106752.9453\n",
            "Epoch 808/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102397.5312 - val_loss: 106745.3047\n",
            "Epoch 809/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102388.6484 - val_loss: 106735.8125\n",
            "Epoch 810/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102381.7188 - val_loss: 106730.1172\n",
            "Epoch 811/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102375.9219 - val_loss: 106720.9688\n",
            "Epoch 812/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102365.3984 - val_loss: 106711.2969\n",
            "Epoch 813/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102360.1016 - val_loss: 106698.8359\n",
            "Epoch 814/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102352.7578 - val_loss: 106696.0469\n",
            "Epoch 815/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102344.1094 - val_loss: 106685.4062\n",
            "Epoch 816/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102337.7109 - val_loss: 106683.3906\n",
            "Epoch 817/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102335.6875 - val_loss: 106670.3906\n",
            "Epoch 818/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102323.5781 - val_loss: 106664.1016\n",
            "Epoch 819/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102314.9297 - val_loss: 106650.7578\n",
            "Epoch 820/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102309.4609 - val_loss: 106643.1797\n",
            "Epoch 821/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102304.9531 - val_loss: 106637.0625\n",
            "Epoch 822/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102297.0703 - val_loss: 106630.7188\n",
            "Epoch 823/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102289.9844 - val_loss: 106614.6719\n",
            "Epoch 824/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102280.9531 - val_loss: 106610.3594\n",
            "Epoch 825/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102273.9375 - val_loss: 106606.2031\n",
            "Epoch 826/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102266.9375 - val_loss: 106596.0469\n",
            "Epoch 827/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102258.1016 - val_loss: 106588.1406\n",
            "Epoch 828/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102254.5156 - val_loss: 106578.1797\n",
            "Epoch 829/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102244.8672 - val_loss: 106576.5391\n",
            "Epoch 830/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102236.4375 - val_loss: 106565.2812\n",
            "Epoch 831/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102232.7500 - val_loss: 106557.1406\n",
            "Epoch 832/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102225.3750 - val_loss: 106551.4453\n",
            "Epoch 833/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102219.0547 - val_loss: 106538.3281\n",
            "Epoch 834/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102211.1562 - val_loss: 106536.1172\n",
            "Epoch 835/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102204.3359 - val_loss: 106528.0234\n",
            "Epoch 836/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102200.3438 - val_loss: 106512.0078\n",
            "Epoch 837/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102192.5547 - val_loss: 106504.9844\n",
            "Epoch 838/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102182.4453 - val_loss: 106493.0156\n",
            "Epoch 839/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102180.7031 - val_loss: 106486.3359\n",
            "Epoch 840/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102174.5781 - val_loss: 106486.6797\n",
            "Epoch 841/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102161.8438 - val_loss: 106479.3594\n",
            "Epoch 842/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102155.1016 - val_loss: 106466.4062\n",
            "Epoch 843/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102151.6562 - val_loss: 106464.4844\n",
            "Epoch 844/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102142.5078 - val_loss: 106457.4375\n",
            "Epoch 845/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102137.5312 - val_loss: 106445.7266\n",
            "Epoch 846/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102127.6719 - val_loss: 106434.8828\n",
            "Epoch 847/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102126.2422 - val_loss: 106426.2188\n",
            "Epoch 848/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102115.2188 - val_loss: 106414.3281\n",
            "Epoch 849/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102108.3125 - val_loss: 106411.1406\n",
            "Epoch 850/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102104.5156 - val_loss: 106394.4453\n",
            "Epoch 851/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102098.9375 - val_loss: 106395.5391\n",
            "Epoch 852/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102089.9844 - val_loss: 106386.2188\n",
            "Epoch 853/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102084.2500 - val_loss: 106372.1250\n",
            "Epoch 854/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102078.3359 - val_loss: 106362.2578\n",
            "Epoch 855/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102071.5391 - val_loss: 106357.0859\n",
            "Epoch 856/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102063.4141 - val_loss: 106348.8672\n",
            "Epoch 857/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102060.3438 - val_loss: 106343.3672\n",
            "Epoch 858/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102054.7578 - val_loss: 106339.9453\n",
            "Epoch 859/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102046.4531 - val_loss: 106331.0938\n",
            "Epoch 860/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102037.9531 - val_loss: 106318.7266\n",
            "Epoch 861/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102034.2109 - val_loss: 106316.5547\n",
            "Epoch 862/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102028.5547 - val_loss: 106305.8125\n",
            "Epoch 863/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102021.8906 - val_loss: 106301.1953\n",
            "Epoch 864/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102015.3594 - val_loss: 106286.3594\n",
            "Epoch 865/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 102009.7188 - val_loss: 106283.6875\n",
            "Epoch 866/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 102006.4062 - val_loss: 106270.8359\n",
            "Epoch 867/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101995.8828 - val_loss: 106267.6484\n",
            "Epoch 868/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101992.8438 - val_loss: 106258.7656\n",
            "Epoch 869/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101984.1641 - val_loss: 106248.6953\n",
            "Epoch 870/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101977.9688 - val_loss: 106242.3125\n",
            "Epoch 871/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101973.4922 - val_loss: 106233.5312\n",
            "Epoch 872/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101963.6641 - val_loss: 106229.5469\n",
            "Epoch 873/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101961.4609 - val_loss: 106216.1719\n",
            "Epoch 874/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101956.0781 - val_loss: 106218.4453\n",
            "Epoch 875/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101947.2500 - val_loss: 106205.3672\n",
            "Epoch 876/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101944.7031 - val_loss: 106202.9844\n",
            "Epoch 877/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101937.6953 - val_loss: 106192.3281\n",
            "Epoch 878/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101935.2422 - val_loss: 106178.3125\n",
            "Epoch 879/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101927.7109 - val_loss: 106175.2031\n",
            "Epoch 880/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101921.2266 - val_loss: 106163.1016\n",
            "Epoch 881/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101913.7578 - val_loss: 106164.1172\n",
            "Epoch 882/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101907.5938 - val_loss: 106156.1094\n",
            "Epoch 883/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101906.5938 - val_loss: 106135.7891\n",
            "Epoch 884/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101902.3203 - val_loss: 106135.9141\n",
            "Epoch 885/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101892.8281 - val_loss: 106126.0547\n",
            "Epoch 886/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101888.2500 - val_loss: 106119.4844\n",
            "Epoch 887/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101883.5859 - val_loss: 106122.1094\n",
            "Epoch 888/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101879.6484 - val_loss: 106110.3594\n",
            "Epoch 889/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101868.9375 - val_loss: 106105.1562\n",
            "Epoch 890/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101867.6328 - val_loss: 106093.9766\n",
            "Epoch 891/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101859.9922 - val_loss: 106091.6875\n",
            "Epoch 892/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101852.4141 - val_loss: 106082.3906\n",
            "Epoch 893/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101849.1953 - val_loss: 106082.6328\n",
            "Epoch 894/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101846.2344 - val_loss: 106069.1797\n",
            "Epoch 895/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101836.8906 - val_loss: 106062.2266\n",
            "Epoch 896/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101832.8906 - val_loss: 106056.2109\n",
            "Epoch 897/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101826.3906 - val_loss: 106047.5469\n",
            "Epoch 898/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101821.4766 - val_loss: 106035.3828\n",
            "Epoch 899/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101816.4844 - val_loss: 106030.9609\n",
            "Epoch 900/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101811.3125 - val_loss: 106026.0312\n",
            "Epoch 901/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101807.6094 - val_loss: 106028.1094\n",
            "Epoch 902/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101795.6562 - val_loss: 106016.5547\n",
            "Epoch 903/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101792.4453 - val_loss: 106010.0078\n",
            "Epoch 904/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101786.6953 - val_loss: 106003.7422\n",
            "Epoch 905/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101782.9297 - val_loss: 105988.2969\n",
            "Epoch 906/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101777.2266 - val_loss: 105984.7734\n",
            "Epoch 907/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101767.6094 - val_loss: 105983.4141\n",
            "Epoch 908/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101763.5547 - val_loss: 105973.9141\n",
            "Epoch 909/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101756.5781 - val_loss: 105964.3281\n",
            "Epoch 910/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101752.6875 - val_loss: 105952.1406\n",
            "Epoch 911/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101752.6562 - val_loss: 105945.0312\n",
            "Epoch 912/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101741.5078 - val_loss: 105937.1484\n",
            "Epoch 913/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101736.6172 - val_loss: 105934.0781\n",
            "Epoch 914/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101738.2656 - val_loss: 105921.4844\n",
            "Epoch 915/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101725.7266 - val_loss: 105918.6875\n",
            "Epoch 916/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101719.2188 - val_loss: 105914.2188\n",
            "Epoch 917/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101714.2500 - val_loss: 105907.4844\n",
            "Epoch 918/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101709.0312 - val_loss: 105901.5312\n",
            "Epoch 919/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101707.2188 - val_loss: 105894.5469\n",
            "Epoch 920/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101702.3828 - val_loss: 105893.8125\n",
            "Epoch 921/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101695.4219 - val_loss: 105882.4688\n",
            "Epoch 922/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101692.2031 - val_loss: 105877.3438\n",
            "Epoch 923/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101683.1328 - val_loss: 105871.6016\n",
            "Epoch 924/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101680.0469 - val_loss: 105860.2109\n",
            "Epoch 925/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101673.0312 - val_loss: 105854.0391\n",
            "Epoch 926/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101670.0234 - val_loss: 105850.0547\n",
            "Epoch 927/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101666.6953 - val_loss: 105835.8281\n",
            "Epoch 928/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101661.6719 - val_loss: 105833.9609\n",
            "Epoch 929/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101654.8750 - val_loss: 105828.9844\n",
            "Epoch 930/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101648.8516 - val_loss: 105822.6406\n",
            "Epoch 931/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101647.0938 - val_loss: 105813.7734\n",
            "Epoch 932/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101639.4688 - val_loss: 105801.7422\n",
            "Epoch 933/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101634.7422 - val_loss: 105801.1797\n",
            "Epoch 934/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101627.0078 - val_loss: 105798.6328\n",
            "Epoch 935/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101624.2188 - val_loss: 105792.8672\n",
            "Epoch 936/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101620.3984 - val_loss: 105791.0625\n",
            "Epoch 937/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101614.0469 - val_loss: 105776.0469\n",
            "Epoch 938/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101609.6641 - val_loss: 105769.4766\n",
            "Epoch 939/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101604.1016 - val_loss: 105763.7031\n",
            "Epoch 940/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101602.4688 - val_loss: 105753.3750\n",
            "Epoch 941/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101592.4375 - val_loss: 105755.0625\n",
            "Epoch 942/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101588.8594 - val_loss: 105747.0469\n",
            "Epoch 943/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101586.4062 - val_loss: 105739.6953\n",
            "Epoch 944/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101581.4688 - val_loss: 105725.4766\n",
            "Epoch 945/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101576.7578 - val_loss: 105730.3672\n",
            "Epoch 946/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101565.2031 - val_loss: 105721.9531\n",
            "Epoch 947/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101567.6094 - val_loss: 105715.6406\n",
            "Epoch 948/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101557.5234 - val_loss: 105707.7266\n",
            "Epoch 949/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101556.0781 - val_loss: 105700.8828\n",
            "Epoch 950/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101550.6406 - val_loss: 105693.1797\n",
            "Epoch 951/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101546.0938 - val_loss: 105687.2344\n",
            "Epoch 952/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101541.3438 - val_loss: 105685.7734\n",
            "Epoch 953/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101535.8672 - val_loss: 105675.5312\n",
            "Epoch 954/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101529.8438 - val_loss: 105666.0156\n",
            "Epoch 955/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101528.4297 - val_loss: 105661.0703\n",
            "Epoch 956/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101522.8047 - val_loss: 105659.8750\n",
            "Epoch 957/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101518.9844 - val_loss: 105646.0625\n",
            "Epoch 958/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101512.7656 - val_loss: 105644.3672\n",
            "Epoch 959/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101508.0547 - val_loss: 105645.2344\n",
            "Epoch 960/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101504.5234 - val_loss: 105638.7500\n",
            "Epoch 961/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101500.7734 - val_loss: 105628.2812\n",
            "Epoch 962/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101498.7656 - val_loss: 105617.3359\n",
            "Epoch 963/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101493.2422 - val_loss: 105621.0547\n",
            "Epoch 964/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101485.1641 - val_loss: 105613.4766\n",
            "Epoch 965/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101481.3438 - val_loss: 105595.8438\n",
            "Epoch 966/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101475.8984 - val_loss: 105596.4844\n",
            "Epoch 967/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101471.3359 - val_loss: 105583.8750\n",
            "Epoch 968/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101467.5781 - val_loss: 105583.5469\n",
            "Epoch 969/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101463.5938 - val_loss: 105577.0234\n",
            "Epoch 970/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101456.5156 - val_loss: 105579.8125\n",
            "Epoch 971/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101452.6406 - val_loss: 105570.7734\n",
            "Epoch 972/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101447.5547 - val_loss: 105567.0156\n",
            "Epoch 973/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101442.9688 - val_loss: 105549.9297\n",
            "Epoch 974/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101438.6641 - val_loss: 105546.5547\n",
            "Epoch 975/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101434.5625 - val_loss: 105546.2578\n",
            "Epoch 976/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101429.4141 - val_loss: 105542.2578\n",
            "Epoch 977/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101425.2344 - val_loss: 105534.8516\n",
            "Epoch 978/1000\n",
            "130/130 [==============================] - 0s 3ms/step - loss: 101421.7422 - val_loss: 105529.1094\n",
            "Epoch 979/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101419.4766 - val_loss: 105519.2891\n",
            "Epoch 980/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101415.1953 - val_loss: 105520.6016\n",
            "Epoch 981/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101408.2031 - val_loss: 105506.2500\n",
            "Epoch 982/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101404.8750 - val_loss: 105503.8984\n",
            "Epoch 983/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101395.7109 - val_loss: 105500.7344\n",
            "Epoch 984/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101391.0469 - val_loss: 105494.3438\n",
            "Epoch 985/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101388.5469 - val_loss: 105486.9609\n",
            "Epoch 986/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101384.3047 - val_loss: 105482.9375\n",
            "Epoch 987/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101381.5156 - val_loss: 105475.4141\n",
            "Epoch 988/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101373.9141 - val_loss: 105466.5703\n",
            "Epoch 989/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101370.8594 - val_loss: 105464.8516\n",
            "Epoch 990/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101365.5469 - val_loss: 105461.2578\n",
            "Epoch 991/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101361.0625 - val_loss: 105456.4531\n",
            "Epoch 992/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101360.0391 - val_loss: 105446.1562\n",
            "Epoch 993/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101353.9375 - val_loss: 105437.4766\n",
            "Epoch 994/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101349.0000 - val_loss: 105436.1719\n",
            "Epoch 995/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101347.6328 - val_loss: 105436.0156\n",
            "Epoch 996/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101338.4531 - val_loss: 105430.3828\n",
            "Epoch 997/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101336.2031 - val_loss: 105422.1250\n",
            "Epoch 998/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101330.3438 - val_loss: 105420.7500\n",
            "Epoch 999/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101331.7188 - val_loss: 105412.8203\n",
            "Epoch 1000/1000\n",
            "130/130 [==============================] - 0s 2ms/step - loss: 101327.8359 - val_loss: 105402.7031\n",
            "169/169 [==============================] - 0s 2ms/step - loss: 99728.2500\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=1)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3vc6DpiZVRZ"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Make a simple neural network for predicting the price of homes in California. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZqmBczcMZVRZ",
        "outputId": "da98e1e8-4f49-4a0b-ee35-34c696f10dca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.3252</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.984127</td>\n",
              "      <td>1.023810</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.3014</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.238137</td>\n",
              "      <td>0.971880</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>2.109842</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.2574</td>\n",
              "      <td>52.0</td>\n",
              "      <td>8.288136</td>\n",
              "      <td>1.073446</td>\n",
              "      <td>496.0</td>\n",
              "      <td>2.802260</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.6431</td>\n",
              "      <td>52.0</td>\n",
              "      <td>5.817352</td>\n",
              "      <td>1.073059</td>\n",
              "      <td>558.0</td>\n",
              "      <td>2.547945</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.8462</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.281853</td>\n",
              "      <td>1.081081</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.181467</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
              "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
              "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
              "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
              "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
              "\n",
              "   Longitude  \n",
              "0    -122.23  \n",
              "1    -122.22  \n",
              "2    -122.24  \n",
              "3    -122.25  \n",
              "4    -122.25  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "cal = fetch_california_housing(as_frame=True)\n",
        "Xcal = pd.DataFrame(cal.data)\n",
        "ycal = pd.DataFrame(cal.target)\n",
        "Xcal.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8TxqgdHUZVRa",
        "outputId": "f29561ee-1d8f-4b8a-f19f-b28371799e23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15480, 8)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_cal,  X_test_cal, y_train_cal, y_test_cal = train_test_split(Xcal, ycal)\n",
        "X_train_cal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k0kBmDdOZVRa",
        "outputId": "6bec0963-b4c7-424b-b2df-f9d98e110e79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_2 (Normalizat  (None, 8)                17        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 170\n",
            "Trainable params: 153\n",
            "Non-trainable params: 17\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cal_normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "cal_normalizer.adapt(np.array(X_train_cal))\n",
        "\n",
        "cal_model = Sequential()\n",
        "cal_model.add(cal_normalizer)\n",
        "cal_model.add(Dense(8, input_shape=(8,), activation='relu'))\n",
        "cal_model.add(Dense(8, activation='relu'))\n",
        "cal_model.add(Dense(1))\n",
        "cal_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XmCWLbNdZVRb",
        "outputId": "be1ca8d5-731f-4ff5-cc39-03ac81607ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 2.3362 - val_loss: 1.0323\n",
            "Epoch 2/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 1.0615 - val_loss: 0.7448\n",
            "Epoch 3/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.8352 - val_loss: 0.6534\n",
            "Epoch 4/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.7104 - val_loss: 0.5868\n",
            "Epoch 5/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6138 - val_loss: 0.5332\n",
            "Epoch 6/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5327 - val_loss: 0.4888\n",
            "Epoch 7/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.4783 - val_loss: 0.4560\n",
            "Epoch 8/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.4443 - val_loss: 0.4365\n",
            "Epoch 9/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.4249 - val_loss: 0.4251\n",
            "Epoch 10/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.4133 - val_loss: 0.4135\n",
            "Epoch 11/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.4052 - val_loss: 0.4062\n",
            "Epoch 12/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3979 - val_loss: 0.3990\n",
            "Epoch 13/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3918 - val_loss: 0.3912\n",
            "Epoch 14/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3858 - val_loss: 0.3849\n",
            "Epoch 15/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3826 - val_loss: 0.3838\n",
            "Epoch 16/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3781 - val_loss: 0.3759\n",
            "Epoch 17/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3749 - val_loss: 0.3749\n",
            "Epoch 18/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3723 - val_loss: 0.3763\n",
            "Epoch 19/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3706 - val_loss: 0.3694\n",
            "Epoch 20/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3672 - val_loss: 0.3661\n",
            "Epoch 21/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3657 - val_loss: 0.3632\n",
            "Epoch 22/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3630 - val_loss: 0.3615\n",
            "Epoch 23/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3608 - val_loss: 0.3592\n",
            "Epoch 24/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3602 - val_loss: 0.3561\n",
            "Epoch 25/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3561 - val_loss: 0.3620\n",
            "Epoch 26/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3551 - val_loss: 0.3507\n",
            "Epoch 27/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3525 - val_loss: 0.3506\n",
            "Epoch 28/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3523 - val_loss: 0.3501\n",
            "Epoch 29/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3485 - val_loss: 0.3497\n",
            "Epoch 30/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3473 - val_loss: 0.3435\n",
            "Epoch 31/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3460 - val_loss: 0.3468\n",
            "Epoch 32/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3434 - val_loss: 0.3427\n",
            "Epoch 33/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3423 - val_loss: 0.3398\n",
            "Epoch 34/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3423 - val_loss: 0.3378\n",
            "Epoch 35/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3403 - val_loss: 0.3377\n",
            "Epoch 36/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3377 - val_loss: 0.3398\n",
            "Epoch 37/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3377 - val_loss: 0.3356\n",
            "Epoch 38/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3364 - val_loss: 0.3380\n",
            "Epoch 39/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3365 - val_loss: 0.3365\n",
            "Epoch 40/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3351 - val_loss: 0.3364\n",
            "Epoch 41/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3345 - val_loss: 0.3331\n",
            "Epoch 42/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3333 - val_loss: 0.3373\n",
            "Epoch 43/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3340 - val_loss: 0.3466\n",
            "Epoch 44/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3327 - val_loss: 0.3336\n",
            "Epoch 45/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3325 - val_loss: 0.3323\n",
            "Epoch 46/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3319 - val_loss: 0.3352\n",
            "Epoch 47/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3295 - val_loss: 0.3345\n",
            "Epoch 48/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3336 - val_loss: 0.3341\n",
            "Epoch 49/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3304 - val_loss: 0.3317\n",
            "Epoch 50/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3298 - val_loss: 0.3326\n",
            "Epoch 51/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3314 - val_loss: 0.3333\n",
            "Epoch 52/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3303 - val_loss: 0.3324\n",
            "Epoch 53/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3286 - val_loss: 0.3309\n",
            "Epoch 54/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3306 - val_loss: 0.3311\n",
            "Epoch 55/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3284 - val_loss: 0.3385\n",
            "Epoch 56/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3276 - val_loss: 0.3310\n",
            "Epoch 57/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3274 - val_loss: 0.3309\n",
            "Epoch 58/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3274 - val_loss: 0.3315\n",
            "Epoch 59/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3271 - val_loss: 0.3339\n",
            "Epoch 60/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3263 - val_loss: 0.3290\n",
            "Epoch 61/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3287 - val_loss: 0.3324\n",
            "Epoch 62/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3259 - val_loss: 0.3285\n",
            "Epoch 63/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3287 - val_loss: 0.3294\n",
            "Epoch 64/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3270 - val_loss: 0.3305\n",
            "Epoch 65/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3263 - val_loss: 0.3289\n",
            "Epoch 66/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3254 - val_loss: 0.3285\n",
            "Epoch 67/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3244 - val_loss: 0.3291\n",
            "Epoch 68/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3233 - val_loss: 0.3309\n",
            "Epoch 69/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3244 - val_loss: 0.3291\n",
            "Epoch 70/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3236 - val_loss: 0.3295\n",
            "Epoch 71/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3228 - val_loss: 0.3311\n",
            "Epoch 72/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3248 - val_loss: 0.3315\n",
            "Epoch 73/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3224 - val_loss: 0.3281\n",
            "Epoch 74/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3234 - val_loss: 0.3285\n",
            "Epoch 75/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3222 - val_loss: 0.3301\n",
            "Epoch 76/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3232 - val_loss: 0.3284\n",
            "Epoch 77/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3221 - val_loss: 0.3273\n",
            "Epoch 78/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3205 - val_loss: 0.3289\n",
            "Epoch 79/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3226 - val_loss: 0.3305\n",
            "Epoch 80/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3206 - val_loss: 0.3260\n",
            "Epoch 81/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3231 - val_loss: 0.3260\n",
            "Epoch 82/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3206 - val_loss: 0.3254\n",
            "Epoch 83/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3190 - val_loss: 0.3339\n",
            "Epoch 84/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3195 - val_loss: 0.3303\n",
            "Epoch 85/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3203 - val_loss: 0.3249\n",
            "Epoch 86/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3200 - val_loss: 0.3343\n",
            "Epoch 87/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3209 - val_loss: 0.3312\n",
            "Epoch 88/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3215 - val_loss: 0.3242\n",
            "Epoch 89/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3196 - val_loss: 0.3335\n",
            "Epoch 90/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3188 - val_loss: 0.3281\n",
            "Epoch 91/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3187 - val_loss: 0.3241\n",
            "Epoch 92/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3178 - val_loss: 0.3239\n",
            "Epoch 93/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3181 - val_loss: 0.3244\n",
            "Epoch 94/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3175 - val_loss: 0.3268\n",
            "Epoch 95/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3175 - val_loss: 0.3290\n",
            "Epoch 96/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3174 - val_loss: 0.3299\n",
            "Epoch 97/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3167 - val_loss: 0.3260\n",
            "Epoch 98/100\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.3168 - val_loss: 0.3224\n",
            "Epoch 99/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3171 - val_loss: 0.3237\n",
            "Epoch 100/100\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3181 - val_loss: 0.3242\n",
            "162/162 [==============================] - 0s 2ms/step - loss: 0.3433\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "cal_model.compile(loss='mean_squared_error', optimizer=tf.optimizers.Adam())\n",
        "cal_train_log = cal_model.fit(X_train_cal, y_train_cal, epochs=100, batch_size=50, validation_split=.2, verbose=1)\n",
        "cal_model.evaluate(X_test_cal, y_test_cal)\n",
        "plot_loss(cal_train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUfNO6HeZVRb"
      },
      "source": [
        "## Basics of Overfitting and Underfitting in Neural Networks\n",
        "\n",
        "Just like any other type of model, our primary task in trying to attain an accurate set of predictions is to balance the overfitting and underfitting. In a neural network, the ideas are the same as with standard models, however the tools and their usage can differ slightly. \n",
        "\n",
        "### Add Data\n",
        "\n",
        "Adding data to the training set is the number one way to improve accuracy. As noted above, neural networks are commonly able to acheive very high accuracy levels if provided with very large training sets. For smaller datasets, the probability of a neural network being the best model is much lower than with big data. \n",
        "\n",
        "### Model Capacity\n",
        "\n",
        "The model capacity is the \"size\" of the model - refering to the combination of the number of neurons on each layer and the number of layers. \n",
        "\n",
        "In general the larger a feature set is, the larger a capacity we will need to be able to avoid underfitting and make accurate predictions. However, similar to a decision tree, if the model becomes too large for the data, we are likely to overfit. \n",
        "\n",
        "In big data scenarios (e.g. Google or Tesla training image recognition models) the feature sets can be massive (e.g. a 5 megapixel image is at least 15 million features) so the networks used have a very high capacity. Because there is a lot of training data, the model is able to have a huge capacity, but not overfit. These models can take FOREVER to process (e.g. weeks with the work paralellized on dedicated and fast machines) but they are able to make very accurate predictions since they get all the \"benefits\" of overfitting - predictions highly tailored to the training data; along with all the \"benefits\" of underfitting - since there is so much training data, they are still generalized enough to predict new data. \n",
        "\n",
        "The combination of large datasets, deep networks, and fast processing allows for most of the modern AI that we see or interact with. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OZWfGHNqZVRb",
        "outputId": "1e595b6c-0e69-477e-dc67-c505ef26ba74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_3 (Normalizat  (None, 18)               37        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               2432      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,134\n",
            "Trainable params: 52,097\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Test Different Model Capacities\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(128, input_dim=18, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WXjXrQEeZVRb",
        "outputId": "7d25815a-6f32-4c0d-c89b-a255ff13816f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 0s 2ms/step - loss: 70572.5469\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=1000, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTocTQASZVRc"
      },
      "source": [
        "### Early Stopping\n",
        "\n",
        "Early stopping is very common with neural networks, due to the common pattern mentioned above of the optimal balance of over/under fitting occuring at some point within many, potentially thousands, of epochs. Early stopping kills the process after it detects that validation loss is going back up. \n",
        "\n",
        "We can put early stopping in place by using a Keras function called a callback, which has odd syntax, but is quite simple to use. The patience pararmeter controls how many epcohs of worsening scores are tolerated before implementing the stop. The restore_best_weights tells the model to roll back all of its weights to the optimal point - so we automatically get the best model post-training. \n",
        "\n",
        "In most cases we probabyl want to use early stopping along with a high epoch number. We can let the model train, and just tell us when it is finished. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b0MYLfTBZVRc"
      },
      "outputs": [],
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(18, input_dim=18, activation='relu'))\n",
        "model.add(Dense(18, activation='relu'))\n",
        "model.add(Dense(18, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SFlOi1yGZVRc",
        "outputId": "256f7158-b58b-48b4-d17e-877c40d84201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 0s 2ms/step - loss: 94271.6484\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD4CAYAAADy46FuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0UElEQVR4nO3de5hcVZno/++7q6qrr+kknaTppJFEiJdcxiBNxOFnbMUnYdQjOAMSRyVqzsRBVPSnM8qM/kBijjqMco4zwDw45BAQJTmohxyEwQzQw2EMuWkwhHAJt9BJSEK6k/Stuqp2vb8/1qru3d1VnUrn0rm8n+epp6revdeuVauTemuttWsvUVWMMcaYYoLRroAxxpiTmyUKY4wxw7JEYYwxZliWKIwxxgzLEoUxxphhxUe7AsfahAkTdOrUqSMu39XVRVVV1bGr0GnE2qY4a5virG2KO5naZtOmTW+q6sRC2067RDF16lQ2btw44vItLS00NzcfuwqdRqxtirO2Kc7apriTqW1E5LVi22zoyRhjzLAsURhjjBmWJQpjjDHDOu3mKIwxZ6ZMJkNrayupVGq0q1Ky2tpatm3bdkJfs7y8nMbGRhKJRMllSkoUIvIq0AGEQFZVm0RkPLASmAq8CnxCVdv9/tcDi/3+X1HVR3z8AuAuoAJ4CLhOVVVEksDdwAXAfuAqVX3Vl1kEfNtX5XuquqLkd2eMOWO0trZSU1PD1KlTEZHRrk5JOjo6qKmpOWGvp6rs37+f1tZWpk2bVnK5Ixl6+oCqzlHVJv/8W8CjqjodeNQ/R0RmAAuBmcClwG0iEvNlbgeWANP97VIfXwy0q+p5wC3AD/2xxgM3AO8B5gI3iMi4I6izMeYMkUqlqKurO2WSxGgQEerq6o6413U0cxSXAflv9yuAyyPx+1S1V1VfAbYDc0WkARijqmvVXbL27kFl8se6H7hE3F97AbBGVdt8b2UN/cnFGGMGsCRxeCNpo1IThQK/FZFNIrLEx+pVdTeAv5/k41OA1yNlW31sin88OD6gjKpmgYNA3TDHOua6erP86LfP89KB8Hgc3hhjTlmlTmZfrKq7RGQSsEZEnhtm30LpSoeJj7RM/wu65LUEoL6+npaWlmGqV9ihXuWfHu/myrfqiMqfCTo7O61tirC2Ke5EtU1tbS0dHR3H/XWG09DQwO7du0vePwzDUalzKpU6or9JSYlCVXf5+70i8mvcfMEeEWlQ1d1+WGmv370VODtSvBHY5eONBeLRMq0iEgdqgTYfbx5UpqVA/e4A7gBoamrSkfzSsa0rDY+vIZlMnjS/lDzZnEy/Ij3ZWNsUd6LaZtu2bSd0YriYI6nDiZ7MzisvL+f8888vef/DDj2JSJWI1OQfA/OBZ4DVwCK/2yLgAf94NbBQRJIiMg03ab3eD091iMhFfv7h6kFl8se6AnjMz2M8AswXkXF+Enu+jx1z+a6LLfhnjDlaqsrf/M3fMGvWLGbPns3KlSsB2L17N/PmzWPOnDnMmjWL3/3ud4RhyGc/+9m+fW+55ZZRrv1QpfQo6oFf+wmQOPBzVf03EdkArBKRxcAO4EoAVd0qIquAZ4EscK2q5gf+r6H/9NiH/Q3gTuAeEdmO60ks9MdqE5GlwAa/302q2nYU77eo/PyO5QljTn3f/T9beXbXoWN6zBmTx3DDf5lZ0r6/+tWv2Lx5M08//TRvvvkmF154IfPmzePnP/85CxYs4O///u8Jw5A9e/awefNmdu7cyTPPPAPAgQMHjmm9j4XDJgpVfRl4V4H4fuCSImWWAcsKxDcCswrEU/hEU2DbcmD54ep5tMT3KSxRGGOO1pNPPsknP/lJYrEY9fX1vP/972fDhg1ceOGFfP7znyeTyXD55Zdz7rnnUlFRwcsvv8yXv/xlPvKRjzB//vzRrv4Q9stsT/wgnA09GXPqK/Wb//GiRT5I5s2bxxNPPMFvfvMbPvOZz/ClL32JL3zhCzz99NM88sgj3HrrraxatYrly4/7d+MjYtd68vrmKEa1FsaY08G8efNYuXIlYRiyb98+nnjiCebOnctrr73GpEmT+Ku/+isWL17cNzSVy+X4i7/4C5YuXcrvf//70a7+ENaj8OyHOsaYY+XjH/84a9eu5V3vehciwj/8wz9w1llnsWLFCm6++WYSiQTV1dXcdttt7Ny5k8997nPkcjkAvv/9749y7YeyROEFfZPZ1qcwxoxMZ2cn4L543nzzzdx8880Dti9atIhFixb1Pc+fHnsy9iKibOjJ65vMtjxhjDEDWKLw7PRYY4wpzBLFYJYpjDFmAEsUXiD2OwpjjCnEEoVnQ0/GGFOYJQrPrvVkjDGFWaLw7HcUxhhTmCUKL7ChJ2PMCVRdXV1026uvvsqsWUMuizdqLFF4+R6FDT0ZY8xA9svsQSxPGHMaePhb8MaWY3vMs2bDn/2g6OZvfvObnHPOOXzxi18E4MYbb0REeOKJJ2hvbyeTyfC9732Pyy677IheNpVKcc0117Bx40bi8Tg//vGP+cAHPsDWrVv53Oc+RzqdJpfL8ctf/pLJkyfziU98gtbWVsIw5Dvf+Q5XXXXVUb1tsEQxQCCWKIwxI7Nw4UK++tWv9iWKVatW8W//9m987WtfY8yYMbz55ptcdNFFfOxjHzuiOdFbb70VgC1btvDcc88xf/58XnjhBf7lX/6F6667jk996lOk02nCMOShhx5i8uTJ/OY3vwHg4MGDx+S9WaKIEBEbejLmdDDMN//j5fzzz2fv3r3s2rWLffv2MW7cOBoaGvja177GE088QRAE7Ny5kz179nDWWWeVfNwnn3ySL3/5ywC84x3v4JxzzuGFF17gve99L8uWLaO1tZU///M/Z/r06cyePZtvfOMbfPOb3+SjH/0o73vf+47Jeyt5jkJEYiLyBxF50D+/UUR2ishmf/twZN/rRWS7iDwvIgsi8QtEZIvf9hO/JCp+2dSVPr5ORKZGyiwSkRf9bRHHkZ33ZIw5GldccQX3338/K1euZOHChdx7773s27ePTZs2sXnzZurr60mlUkd0zGJrW/zlX/4lq1evpqKiggULFvDYY4/xtre9jU2bNjF79myuv/56brrppmPxto5oMvs6YNug2C2qOsffHgIQkRm4pUxnApcCt4lIzO9/O7AEt472dL8dYDHQrqrnAbcAP/THGg/cALwHmAvc4NfOPi5EbDLbGDNyCxcu5L777uP+++/niiuu4ODBg0yaNIlEIsHjjz/Oa6+9dsTHnDdvHvfeey8AL7zwAjt27ODtb387L7/8Mm9961v5yle+wsc+9jH++Mc/smvXLiorK/n0pz/NN77xjWN2VdqSEoWINAIfAf61hN0vA+5T1V5VfQXYDswVkQZgjKquVZci7wYuj5RZ4R/fD1ziexsLgDWq2qaq7cAa+pPLMSciNkdhjBmxmTNn0tHRwZQpU2hoaOBTn/oUGzdupKmpiXvvvZd3vOMdR3zML37xi4RhyOzZs7nqqqu46667SCaTrFy5klmzZjFnzhyee+45rr76arZs2cLcuXOZM2cOy5Yt49vf/vYxeV9SrFszYCeR+4HvAzXAN1T1oyJyI/BZ4BCwEfi6qraLyD8DT6nqz3zZO4GHgVeBH6jqh3z8fcA3/bGeAS5V1Va/7SVcL+KzQLmqfs/HvwP0qOo/DqrfElxPhfr6+gvuu+++ETXGX/22i/c3KJ+eXfz85jNZZ2fnsOd+n8msbYo7UW1TW1vLeeedd9xf51gKw5BYLHb4HY+x7du3D5no/sAHPrBJVZsK7X/YyWwR+SiwV1U3iUhzZNPtwFLciUJLgR8Bn6fwUL8OE2eEZfoDqncAdwA0NTVpc3Pz4F1KEnv0YeKJGCMtf7praWmxtinC2qa4E9U227Zto6am5ri/zrGUX7joRCsvL+f8888vef9Sznq6GPiYn6wuB8aIyM9U9dP5HUTkp8CD/mkrcHakfCOwy8cbC8SjZVpFJA7UAm0+3jyoTEspb2wk3OJFNvhkjDkxtm7dyl//9V8PiCWTSdatWzdKNSrssIlCVa8HrgfwPYpvqOqnRaRBVXf73T4OPOMfrwZ+LiI/BibjJq3Xq2ooIh0ichGwDrga+KdImUXAWuAK4DFVVRF5BPhvkQns+fm6HA/2OwpjTm2qekpdt23mzJls3rz5hL5mKdMNgx3N7yj+QUTm4D5bXwW+4CuxVURWAc8CWeBaVQ19mWuAu4AK3LzFwz5+J3CPiGzH9SQW+mO1ichSYIPf7yZVbTuKOg/LfkdhzKmrvLyc/fv3U1dXd0olixNJVdm/fz/l5eVHVO6IEoWqtuCHflT1M8PstwxYViC+ERhypStVTQFXFjnWcmD5kdRzpGzgyZhTV2NjI62trezbt2+0q1KyVCp1xB/aR6u8vJzGxsbD7xhhv8yOsC8hxpy6EokE06ZNG+1qHJGWlpYjmlQeLXb12AgRIWddCmOMGcASRYT1KIwxZihLFBE2R2GMMUNZoogIxM6PNcaYwSxRRIhAbrQrYYwxJxlLFANYj8IYYwazRBFhI0/GGDOUJYoIu4SHMcYMZYkiQrBLeBhjzGCWKCLsdxTGGDOUJYqIwFa4M8aYISxRDGJDT8YYM5Aligg768kYY4ayRBHhEoWlCmOMiSo5UYhITET+ICIP+ufjRWSNiLzo78dF9r1eRLaLyPMisiASv0BEtvhtPxG/uoiIJEVkpY+vE5GpkTKL/Gu8KCKLjsm7LsIu4WGMMUMdSY/iOmBb5Pm3gEdVdTrwqH+OiMzArVA3E7gUuE1EYr7M7cAS3PKo0/12gMVAu6qeB9wC/NAfazxwA/AeYC5wQzQhHWt2UUBjjBmqpEQhIo3AR4B/jYQvA1b4xyuAyyPx+1S1V1VfAbYDc0WkARijqmvVLdp696Ay+WPdD1ziexsLgDWq2qaq7cAa+pPLMWdLoRpjzFClrnD334G/BWoisXpV3Q2gqrtFZJKPTwGeiuzX6mMZ/3hwPF/mdX+srIgcBOqi8QJl+ojIElxPhfr6elpaWkp8WwP1dHeTqcyNuPzprrOz09qmCGub4qxtijtV2uawiUJEPgrsVdVNItJcwjEL/WxNh4mPtEx/QPUO4A6ApqYmbW4upZpDVW1qIR5LMdLyp7uWlhZrmyKsbYqztinuVGmbUoaeLgY+JiKvAvcBHxSRnwF7/HAS/n6v378VODtSvhHY5eONBeIDyohIHKgF2oY51nFhS6EaY8xQh00Uqnq9qjaq6lTcJPVjqvppYDWQPwtpEfCAf7waWOjPZJqGm7Re74epOkTkIj//cPWgMvljXeFfQ4FHgPkiMs5PYs/3sePCruBhjDFDlTpHUcgPgFUishjYAVwJoKpbRWQV8CyQBa5V1dCXuQa4C6gAHvY3gDuBe0RkO64nsdAfq01ElgIb/H43qWrbUdR5WHYJD2OMGeqIEoWqtgAt/vF+4JIi+y0DlhWIbwRmFYin8ImmwLblwPIjqedIiYDaEnfGGDOA/TJ7EOtRGGPMQJYoIsSuM26MMUNYoogIxK4ea4wxg1miiLBLPRljzFCWKCIEO+vJGGMGs0QRIXZVQGOMGcISRYTY7yiMMWYISxQRgk1mG2PMYJYoImyFO2OMGcoSRURgv6MwxpghLFFECNjVY40xZhBLFBHWoTDGmKEsUUTY7yiMMWYoSxQRYpfwMMaYISxRRNglPIwxZihLFBFia9wZY8wQh00UIlIuIutF5GkR2Soi3/XxG0Vkp4hs9rcPR8pcLyLbReR5EVkQiV8gIlv8tp/4JVHxy6au9PF1IjI1UmaRiLzob4s4jmzoyRhjhiplhbte4IOq2ikiCeBJEckvYXqLqv5jdGcRmYFbynQmMBn4dxF5m18O9XZgCfAU8BBwKW451MVAu6qeJyILgR8CV4nIeOAGoAk3KrRJRFaravvRve3CbClUY4wZ6rA9CnU6/dOEvw33eXoZcJ+q9qrqK8B2YK6INABjVHWtqipwN3B5pMwK//h+4BLf21gArFHVNp8c1uCSy3FhPQpjjBmqpDWzRSQGbALOA25V1XUi8mfAl0TkamAj8HX/YT4F12PIa/WxjH88OI6/fx1AVbMichCoi8YLlInWbwmup0J9fT0tLS2lvK0h2ttShLlwxOVPd52dndY2RVjbFGdtU9yp0jYlJQo/bDRHRMYCvxaRWbhhpKW43sVS4EfA56HgjLAOE2eEZaL1uwO4A6CpqUmbm5uHeTfF/c+X19O9Zz8jLX+6a2lpsbYpwtqmOGub4k6Vtjmis55U9QDQAlyqqntUNVTVHPBTYK7frRU4O1KsEdjl440F4gPKiEgcqAXahjnWcWFDT8YYM1QpZz1N9D0JRKQC+BDwnJ9zyPs48Ix/vBpY6M9kmgZMB9ar6m6gQ0Qu8vMPVwMPRMrkz2i6AnjMz2M8AswXkXEiMg6Y72PHha1bZIwxQ5Uy9NQArPDzFAGwSlUfFJF7RGQO7rP1VeALAKq6VURWAc8CWeBaP3QFcA1wF1CBO9spf/bUncA9IrId15NY6I/VJiJLgQ1+v5tUtW3kb3d4tnCRMcYMddhEoap/BM4vEP/MMGWWAcsKxDcCswrEU8CVRY61HFh+uHoeC4H93s4YY4awX2YPIHaZcWOMGcQSRYRdZtwYY4ayRBHh1sy2LoUxxkRZooiwpVCNMWYoSxQRIpAb7UoYY8xJxhJFhNgPKYwxZghLFBG2FKoxxgxliSLCLuFhjDFDWaKIsF9mG2PMUJYoIuycJ2OMGcoSRUQgNpdtjDGDWaKIEBGbozDGmEEsUUTY2bHGGDNUSSvcnRHSXVyy/2fs0amjXRNjjDmpWI8iL9PDR/b+lNlsH+2aGGPMSaWUFe7KRWS9iDwtIltF5Ls+Pl5E1ojIi/5+XKTM9SKyXUSeF5EFkfgFIrLFb/uJX+kOvxreSh9fJyJTI2UW+dd4UUQWcbyIa4rALuJhjDEDlNKj6AU+qKrvAuYAl4rIRcC3gEdVdTrwqH+OiMzArVA3E7gUuM2vjgdwO7AEtzzqdL8dYDHQrqrnAbcAP/THGg/cALwHtyb3DdGEdEz5RCFqicIYY6IOmyjU6fRPE/6mwGXACh9fAVzuH18G3Keqvar6CrAdmOvX2B6jqmv9eth3DyqTP9b9wCW+t7EAWKOqbaraDqyhP7kcW9ajMMaYgkqazPY9gk3AecCtqrpOROpVdTeAqu4WkUl+9ynAU5HirT6W8Y8Hx/NlXvfHyorIQaAuGi9QJlq/JbieCvX19bS0tJTytgYIwhTzgEDDEZU/E3R2dlrbFGFtU5y1TXGnStuUlChUNQTmiMhY4NciMmTd64hCP3DWYeIjLROt3x3AHQBNTU3a3Nw8TPWKyKTg/7oXHFH5M0BLS4u1TRHWNsVZ2xR3qrTNEZ31pKoHgBbc8M8eP5yEv9/rd2sFzo4UawR2+XhjgfiAMiISB2qBtmGOdez5oaeYDT0ZY8wApZz1NNH3JBCRCuBDwHPAaiB/FtIi4AH/eDWw0J/JNA03ab3eD1N1iMhFfv7h6kFl8se6AnjMz2M8AswXkXF+Enu+jx17gZtvF0sUxhgzQClDTw3ACj9PEQCrVPVBEVkLrBKRxcAO4EoAVd0qIquAZ4EscK0fugK4BrgLqAAe9jeAO4F7RGQ7riex0B+rTUSWAhv8fjepatvRvOGibDLbGGMKOmyiUNU/AucXiO8HLilSZhmwrEB8IzBkfkNVU/hEU2DbcmD54ep51ETIIZYojDFmEPtldoQS2Bp3xhgziCWKCJXAehTGGDOIJYoIJSCwHoUxxgxgiSJCJSCwS3gYY8wAligicmJzFMYYM5gligg39GQ9CmOMibJEEaES2C+zjTFmEEsUEe70WEsUxhgTZYkiwk6PNcaYoSxRRKhNZhtjzBCWKCIUm6MwxpjBLFFE5GzoyRhjhrBEEaESs19mG2PMIJYoBrCrxxpjzGCWKCJy1qMwxpghSlnh7mwReVxEtonIVhG5zsdvFJGdIrLZ3z4cKXO9iGwXkedFZEEkfoGIbPHbfuJXusOvhrfSx9eJyNRImUUi8qK/LeK4cj0Kt7ieMcYYKG2FuyzwdVX9vYjUAJtEZI3fdouq/mN0ZxGZgVuhbiYwGfh3EXmbX+XudmAJ8BTwEG7t7YeBxUC7qp4nIguBHwJXich44AagCVD/2qtVtf3o3nZhKjFi5FAFl8KMMcYctkehqrtV9ff+cQewDZgyTJHLgPtUtVdVXwG2A3NFpAEYo6pr/XrYdwOXR8qs8I/vBy7xvY0FwBpVbfPJYQ0uuRwX7gd3Ss56FMYY06eUHkUfPyR0PrAOuBj4kohcDWzE9TracUnkqUixVh/L+MeD4/j71wFUNSsiB4G6aLxAmWi9luB6KtTX19PS0nIkb6vPuekMATla/uM/iAfWpRiss7NzxG17urO2Kc7aprhTpW1KThQiUg38Eviqqh4SkduBpbghoaXAj4DPA4U+YXWYOCMs0x9QvQO4A6CpqUmbm5uHfS/F7NlUSZDKcfG891MWt3n+wVpaWhhp257urG2Ks7Yp7lRpm5I+DUUkgUsS96rqrwBUdY+qhqqaA34KzPW7twJnR4o3Art8vLFAfEAZEYkDtUDbMMc6LhQhRs6GnowxJqKUs54EuBPYpqo/jsQbIrt9HHjGP14NLPRnMk0DpgPrVXU30CEiF/ljXg08ECmTP6PpCuAxP4/xCDBfRMaJyDhgvo8dFyoxu9aTMcYMUsrQ08XAZ4AtIrLZx/4O+KSIzMENBb0KfAFAVbeKyCrgWdwZU9f6M54ArgHuAipwZzs97ON3AveIyHZcT2KhP1abiCwFNvj9blLVtpG80ZKI9J31ZIwxxjlsolDVJyk8V/DQMGWWAcsKxDcCswrEU8CVRY61HFh+uHoeCyoxYpJDrVdhjDF9bMY2In+Z8ZzlCWOM6WOJIiJ/mXH7ZbYxxvSzRBGRv3qspQljjOlniSJK8td6Gu2KGGPMycMSRUT/tZ4sUxhjTJ4liih/rSfLE8YY088SRYT6pVAtTxhjTD9LFBFuMtsu4WGMMVGWKKLsl9nGGDOEJYqI/LWe7ARZY4zpZ4kiStwP7ixPGGNMP0sUEflrPdklPIwxpp8liih/rScbejLGmH6WKCL6r/U02jUxxpiThyWKqMCu9WSMMYNZoohQf62nnE1SGGNMn1KWQj1bRB4XkW0islVErvPx8SKyRkRe9PfjImWuF5HtIvK8iCyIxC8QkS1+20/8kqj4ZVNX+vg6EZkaKbPIv8aLIrKI46nvWk/H9VWMMeaUUkqPIgt8XVXfCVwEXCsiM4BvAY+q6nTgUf8cv20hMBO4FLhNRGL+WLcDS3DraE/32wEWA+2qeh5wC/BDf6zxwA3Ae4C5wA3RhHSsBYG7hEcmlzteL2GMMaecwyYKVd2tqr/3jzuAbcAU4DJghd9tBXC5f3wZcJ+q9qrqK8B2YK6INABjVHWtusuz3j2oTP5Y9wOX+N7GAmCNqrapajuwhv7kcsxJECdAyYSWKIwxJu+wa2ZH+SGh84F1QL2q7gaXTERkkt9tCvBUpFirj2X848HxfJnX/bGyInIQqIvGC5SJ1msJrqdCfX09LS0tR/K2+lS3tTOJHE+t28AbtbHDFzjDdHZ2jrhtT3fWNsVZ2xR3qrRNyYlCRKqBXwJfVdVDfnqh4K4FYjpMfKRl+gOqdwB3ADQ1NWlzc3Oxug1rxxv/h9j+HH9y/rt591uO2wjXKaulpYWRtu3pztqmOGub4k6VtinprCcRSeCSxL2q+isf3uOHk/D3e328FTg7UrwR2OXjjQXiA8qISByoBdqGOdZxEcTiCEo2tNlsY4zJK+WsJwHuBLap6o8jm1YD+bOQFgEPROIL/ZlM03CT1uv9MFWHiFzkj3n1oDL5Y10BPObnMR4B5ovIOD+JPd/HjgsJ3A/ubI7CGGP6lTL0dDHwGWCLiGz2sb8DfgCsEpHFwA7gSgBV3Soiq4BncWdMXauqoS93DXAXUAE87G/gEtE9IrId15NY6I/VJiJLgQ1+v5tUtW1kb/XwglicuORIW6Iwxpg+h00UqvokhecKAC4pUmYZsKxAfCMwq0A8hU80BbYtB5Yfrp7HQhC4Cexs1hKFMcbk2S+zI4KYa45sNjPKNTHGmJOHJYqIIHAdrEyYHeWaGGPMycMSRUQslh96Cg+zpzHGnDksUUQEPlGEWetRGGNMniWKiJifzA5t6MkYY/pYoogIYn6OwoaejDGmjyWKiL45CutRGGNMH0sUEflEkbM5CmOM6WOJIqJv6Cm0oSdjjMmzRBEh4pojZ0NPxhjTxxJFVGCnxxpjzGCWKKIkf3qsDT0ZY0yeJYqoeNLdZ1OjWw9jjDmJWKKISlQCINmeUa6IMcacPCxRRJW5RBFYojDGmD6lrHC3XET2isgzkdiNIrJTRDb724cj264Xke0i8ryILIjELxCRLX7bT/wqd/iV8Fb6+DoRmRops0hEXvS3/Ap4x0/CEoUxxgxWSo/iLuDSAvFbVHWOvz0EICIzcKvTzfRlbhPxM8RwO7AEtzTq9MgxFwPtqnoecAvwQ3+s8cANwHuAucANfjnU48cnili2+7i+jDHGnEoOmyhU9Qnc8qSluAy4T1V7VfUVYDswV0QagDGqutavhX03cHmkzAr/+H7gEt/bWACsUdU2VW0H1lA4YR07iQoA4qFNZhtjTF4pa2YX8yURuRrYCHzdf5hPAZ6K7NPqYxn/eHAcf/86gKpmReQgUBeNFygzgIgswfVWqK+vp6WlZURvqKy3jT8Feg/tH/ExTmednZ3WLkVY2xRnbVPcqdI2I00UtwNLAfX3PwI+T+G1tXWYOCMsMzCoegdwB0BTU5M2NzcPU/VhpA7CWqgpF0Z8jNNYS0uLtUsR1jbFWdsUd6q0zYjOelLVPaoaqmoO+CluDgHct/6zI7s2Art8vLFAfEAZEYkDtbihrmLHOn78HEWZDT0ZY0yfESUKP+eQ93Egf0bUamChP5NpGm7Ser2q7gY6ROQiP/9wNfBApEz+jKYrgMf8PMYjwHwRGecnsef72PETS5AhRjxnicIYY/IOO/QkIr8AmoEJItKKOxOpWUTm4IaCXgW+AKCqW0VkFfAskAWuVdX89TCuwZ1BVQE87G8AdwL3iMh2XE9ioT9Wm4gsBTb4/W5S1VIn1UcsTZKEJQpjjOlz2EShqp8sEL5zmP2XAcsKxDcCswrEU8CVRY61HFh+uDoeSymxRGGMMVH2y+xBMpIkHtrvKIwxJs8SxSCZIEnMLgpojDF9LFEMEgZJkpqiJ22XGjfGGLBEMUQYK6dC0rR3p0e7KsYYc1KwRDFILpakkl4OdGdGuyrGGHNSsEQxSJioZrwc4oD1KIwxBrBEMUSq4izq5QAHOzpGuyrGGHNSsEQxSKbS/eg83P/yKNfEGGNODpYoBgmrzwIgaH9llGtijDEnB0sUg2SrXI8iOPDq6FbEGGNOEpYoBsnGazgQjGPc3nWjXRVjjDkpWKIYTIRtDX/OhekNdL+8drRrY4wxo84SRSEXfYFWnUDi51fAxv8JvZ2jXSNjjBk1ligKmDvzbXy37mZ2Zmvgwa/Cbe+F//wf8Pp6yNh1oIwxZ5ajWTP7tBULhO98aj7/dXkVjYf+wPez/5uGNf+f31gGZ/0JNDbBmMlw7gehthFiSSirHN2KG2PMcVDKwkXLgY8Ce1V1lo+NB1YCU3ELF31CVdv9tuuBxUAIfEVVH/HxC+hfuOgh4DpVVRFJAncDFwD7gatU9VVfZhHwbV+V76nqiqN+xyWaOqGK+7/UzHcemMif/nEWZ8kBPtW4jwW1Ozin+1nKNq2AbA/kE0ii0iUQEagYB2PfAvWz4MXfQjwJs/4C9m+HsiqY/Ql3796k66UkyodWQrV/H2OMGSWl9CjuAv4Z92Ge9y3gUVX9gYh8yz//pojMwK1QNxOYDPy7iLzNr3J3O7AEeAqXKC7FrXK3GGhX1fNEZCHwQ+Aqn4xuAJpwK+ltEpHV+YR0IoytLOOfPnk+X/3QdFZueJ0Vf9jJP+6YDlzCW2rL+NPGkAX6O+qTacrT7UzsfJ5keQWJA68hL7dAphvKayGbhi3/q//AD/6/7i3FK6BiLHTshrNmw9hz3PbKOtdbeeZXLoG865PQvR8a5kDVBKicAFV1rhcTT0J3G7R8Hy5YBBPfAZ17YcwU6GmHyvEQZlxdKsaeqKYzxpxGSlnh7gkRmToofBlueVSAFUAL8E0fv09Ve4FX/PKmc0XkVWCMqq4FEJG7gctxieIy4EZ/rPuBf/brai8A1uSXPxWRNbjk8osjf5tH59yJ1fzdh9/J3y54O8/sOsSm19rZ9Fob63d38L/aLiLM6YD9K8tiTK4OmF29h+7ac5lYGeedvESitoHq8AAzutaRTMSpzLSTzBwkNn0C8d2/R3Y8BclqSHdB177+A+5+unjlJAAENISNkYUHJeZiZdWQ9pPxNQ2QqHCJI550+ySroWqSS0S1U0BzkOmB+pkQL4euN2HHWrhwMZP2vA7/+UdXt2nzoOYsCNNuOK7tFZeMEBh3Dox/q+sR7X4axjRAEIfqeuh4A3o7YNI73X33m67n1bnX1XFMA+RCePMFqJsO2VR/76vrTTjUCmMaoXoitL0MVRMhWTOwTYr10ABeeAQ2/xwuv/3IhwpVi/bukqm9kMtBcBJM++Vy7u9SrA1G28nSTqZkI52jqFfV3QCqultEJvn4FFyPIa/VxzL+8eB4vszr/lhZETkI1EXjBcoMICJLcL0V6uvraWlpGeHbgs7OzsOWPxc4d4qrTTZXwb5uJQccSCm7u3Ls7c5xKJ3jhfQEDu08SEda+UV6DKF2AQng/xlyzJh8iOoyQTthTBlUxXsJ4gneFn+D2niWWCzOZN1DEE8wXtuppockGSpJkaSX9uq3Mz69iyRpcmXVlGc7yMWSlKXbSZeNRSWgoucNglwG4kIsTKEEJA+9SXCgjUyimqo9jwGCSozE9scINNtfwQe/xgyAbaAEyO9+MuI2BlAEwSXYbKyCWOhOEsgkxhDkeomH/ScN5CQOCIFmfNmA3uR4ynvfJBurJF1Wi2hIWfoAPRUNVHe9RjpRS6p8onu/BKgE5II4tYeeB6D7lfX0JscTz3aTC8rIxisJcllEswS5DGGsnHi2G8gRxiooT71JInOAA2NnE8YqSGQOIArxbBcqAe/tfIkD226hp2IyVV076E3WkUmMAZRsvJqKnt2Up94gk6glkxjDmEMv0FX1Frqqzqam4yVS5ROBgHi2y7WO5oiFPaTKJ1GWPkiydx8dNeeSyHQQxsoJY+WoxKjo2UNHzVvdv6GwB4Dag89T2b2T/XUX0FNxFmXpduLZbrqq3kJZ+iBl6f2I5mgf9y4SmUMkMh0EuV7SZXVAjlxQRnlqH2XpNlLlkwhjFYSxCkSzJHvbiGc7SZeNJV02jljYS7qslni2m3i2i1jYQyZRC7i2qT24lQl1H+ClHb9m7IEtJHv3k+zdz/66C6js3knb+PN9u+dIlU8gFySJZzsJY0lfrwzpsrEkMp0EuTSgqMRREWJhmiDXSy6IE8YqyMariYU9vg3dkgGiIWMPPEtPRT3dlY3Ewl7KU3v9376LMYeepzdZx/66C8kkaohne4hnO6nqep3qzpfIJMbQUzGZbLyKVPlEkr376ayeRjzbTSzsIRuvIoyVU5Y+4P+9QN3+9RysnUFX1VtIZA5RntqLaI5E5gA9FVNIlU9ANIdojvpUN69vX05Z+kDfv8d4tpNDY94OCLGwh1jYg6hSlm4nG69ENEdZup1U+URS5fWoBFR276Q3OYF02VjeaPjQUf3fLERU9fA7uR7Fg5E5igOqOjayvV1Vx4nIrcBaVf2Zj9+JG2baAXxfVT/k4+8D/lZV/4uIbAUWqGqr3/YSMBf4PJBU1e/5+HeAblX90XB1bWpq0o0bNx5JGwzQ0tJCc3PziMsXo6ocSmXpzYbs2N9NRyrLoVSGjlSWzt4sHakMb3akCQLY15FGVenozdLelaa9O0N3Okv3ESymFAuEyrIYFYkY5YkY5YnA37tbMh74W4xkov9xdTLGuKoywpxSXSYQZlCJkezcyaFdL3Huu+dRUV7BWe2bIN1JqEqF9lJ19p9AvIy4ZpCDrW44recA1M9wPYRMt+uJ1DS4Xk7rBtcTqJoIu/7geg2JSlculnD79Ha4EwV62l3vqPYtUD0J9myF9lfd49RB1wNCXW+p/TWYMB1yWdj/EpSPddtyWdeTSlS4+aOdmyBIuDpkU+61YmXutYO462FVjne9rnSnq0fXPtj9RxevnNDfYwsz7O/spY4D0NMGFeNdnXraXa+lez9Un+V6b9k05DIw+d2uDp17XO+r95DrHcaT0N3u5rmq6uDA667ek94Juze7uodp1+tM+6HNjl2AuLqAO0ZlnXtPHbtcGwdx17Zx38sor3WvLbH+HlnqIAQx11YV41ydu/a608PDXrePBO7Y2V5X5yDu9gdIjnGv3bXP7ae5gf8ox01zPbJ0l2ubIO7+XcSSLl5wZUlxfz8J3P7gXk/V/XtJVLhjZCLLFyeqXJn88cZMce+t95B7XjPZtUu8wr3PbMr93aKqJro5x0M73d8g01Xi/zwgWevqk/NLFZTVuOdj3+KOFw66MrUEri7ZHve4Yrxr97x4uWvLfHuBG6buboO0v3hpvvzkd8OSx0uva7QaIptUtanQtpH2KPaISIPvTTQA+XfVCpwd2a8R2OXjjQXi0TKtIhIHaoE2H28eVKZlhPUddSJCbUUCSDCpZmRDArmc0pMJ6Upn6e719+mQrt4sXfnnvVm60iHdaRfrSYf0ZkNSmRypbEgqE3KwJ0NvJiQd5ujN5OjN5ujNhvRmc6SzuWFqMAG2PBt57j+YqCH/5wwEyhNxyhNTKY8HlJfFKI/HKE/Usa9zEokgYGxlgnjsQ9Qk475t3kYgQjwmTKxOUhYExDIB8biQSPn7mBDPBMQPCfGqdxMf42KZUDnUk6GuuoyKRIxABMkPD53nPmbcawy6n5Z/MOCur6wAVcm4O2YAFYkYsUDoSYdUl8fJhko6zBHmFFVYv2EDcy+8kGQ8IBAhHYaMrSwjp4qGWSQWJxAhJuLqGEBclN5UilwsSW82R1k8IBEL+npafZUtMuRVHo+RiAlhNkNWhVCFRCwgERPyo6FBLk2aON2pLF09nVSUV5GIQSyIUdm7l1z5WHLxCtKZkLK4EA+E7lSaWKKMZDzoa4+Orm4qgpBYooysBiRiMcKc+zdTnushKK9xSQZ84vYJoWIca3/7Sy66eB5SUz+g/mGYozedorKiEs3lkJ52SB1wH9LpLnIVdSABkmrjYDZBzZixxAT3oSnBwPZRhd6D7kM5VuBjLdPj6pOsccmsc59LErG4K3top0uI5WNcwk3WDGzzzr3uC0PNZNj3nPuykKyB1CE//zfefdnpaae74izKgxxy8HWkaoKbGwwz7ktImHVJKYiDCP/x5O+Yd8mlSP5foOZcO3a96epWVu3KQf+QYhBzMVWXcMOM+9LU2zF0GPYYGWmiWA0sAn7g7x+IxH8uIj/GTWZPB9araigiHSJyEbAOuBr4p0HHWgtcATzmz4Z6BPhvIjLO7zcfuH6E9T0tBIFQlYxTlYy7z+bjoCcdciiVIRChqzfrvujllLJYwIOPr2XOnDl09WY50JMhENdzOdCdoSOVcf9XszlSmZBUNqQn7ZJTb8YlqhkNSQShrTtNLqfs6XDf+HI5yKn74P2/HW+SDZVQlWyYI3f4Du/J4z+fGO0aHJWyWICiZELX6Pm/bywQUpkcgdD398gnaXCfp5WJGJmckowF9GRCKsti9GZz7sQ9rSTzHxupLosTiwk5n1yz/otPMh70JcpkLCDUF8mpklPIhDnigXut8kTg6qj0bc+p9m1X1CXjoP8WD9wXB1X3vnI5HVC2/1iKAuMry/q+QCEQ88fLv8+KxMv0pENy+gYKjK1IkFPoybxOVVmMnkxIe/cfiQVCMh4wvmoHqYz7IpaMx4j7YymuDu2dMHbdYyTjMTJhjoz/8hElkYQVzV2JWECVb+cw5977nLPH8i+fueCY/9so5fTYX+C+2U8QkVbcmUg/AFaJyGLcsNKVAKq6VURWAc8CWeBaf8YTwDX0nx77sL8B3Anc4ye+23BnTaGqbSKyFNjg97spP7Ftjp+KshgVZe6b4cSa5IBt76yL8d5z605ofcKckglzZHMucWRCJZvLkQ2VbE4RYFxlGW3daVKZcMB/svyoqvpv6P3P89t10PO+kgC0dWXIhK6H1ZMOyYQ5KpNxOlNZ4jGhLBYQC1wP4dlntzJjxkzSYUg2VMriAe1daWKxgJhI34eR+6ByH07ZnPuQiwdCecJ9UPQO26MbqLM3Sy6nxGOuJxKI+2BNh0pMBJH+D9J876gnE5LOuvZMZ3P+gxXiMVffIHA935wq3b0hoa9zdTJObzZH4Ovblc5SmYhTFg/oSWfp7A2Jx4RMmKM8EaOrN0u57+G9+toOzpt2jquvqu/1ufaeWJPkQHeaikSMXv8BHQ+EIHAf7hWJGOlQqasq441Dqb4PxEDcFycRyGSVRNz11sKcEuZc2+bbWFURERKBEAuCAWX7juX/Rm1dGZJxN0yruPce5twHdJhTetIhlclYX4I51JNxCS7u3nMyEWNidRnp0LVve3e6b6i3N5tzSRLt6/m27d3FmLpJKC75ur9j4RMmBk8T9GZzdKVdok3E3HufPLai5H8/R6KUs54+WWTTJUX2XwYsKxDfCMwqEE/hE02BbcuB5Yerozl9uW+GscPuV1uZOAG1Ka6q7Xma/6RhVOtwsmppeYPm5rePdjVOSi0t+2luftdoV+Ow7Bw1Y4wxw7JEYYwxZliWKIwxxgzLEoUxxphhWaIwxhgzLEsUxhhjhmWJwhhjzLAsURhjjBlWSRcFPJWIyD7gtaM4xATgzWNUndONtU1x1jbFWdsUdzK1zTmqOrHQhtMuURwtEdlY7AqKZzprm+KsbYqztinuVGkbG3oyxhgzLEsUxhhjhmWJYqg7RrsCJzFrm+KsbYqztinulGgbm6MwxhgzLOtRGGOMGZYlCmOMMcOyROGJyKUi8ryIbBeRb412fU40EVkuIntF5JlIbLyIrBGRF/39uMi2631bPS8iC0an1ieGiJwtIo+LyDYR2Soi1/n4Gd8+IlIuIutF5GnfNt/18TO+bfJEJCYifxCRB/3zU65tLFHg/pDArcCfATOAT4rIjNGt1Ql3F3DpoNi3gEdVdTrwqH+Ob5uFwExf5jbfhqerLPB1VX0ncBFwrW8Dax/oBT6oqu8C5gCXishFWNtEXQdsizw/5drGEoUzF9iuqi+rahq4D7hslOt0QqnqE7g1y6MuA1b4xyuAyyPx+1S1V1VfAbbj2vC0pKq7VfX3/nEH7j/9FKx9UKfTP034m2JtA4CINAIfAf41Ej7l2sYShTMFeD3yvNXHznT1qrob3IclMMnHz9j2EpGpwPnAOqx9gL6hlc3AXmCNqlrb9PvvwN8CuUjslGsbSxSOFIjZecPFnZHtJSLVwC+Br6rqoeF2LRA7bdtHVUNVnQM0AnNFZNYwu58xbSMiHwX2quqmUosUiJ0UbWOJwmkFzo48bwR2jVJdTiZ7RKQBwN/v9fEzrr1EJIFLEveq6q982NonQlUPAC248XVrG7gY+JiIvIobzv6giPyMU7BtLFE4G4DpIjJNRMpwE0qrR7lOJ4PVwCL/eBHwQCS+UESSIjINmA6sH4X6nRAiIsCdwDZV/XFk0xnfPiIyUUTG+scVwIeA57C2QVWvV9VGVZ2K+0x5TFU/zSnYNvHRrsDJQFWzIvIl4BEgBixX1a2jXK0TSkR+ATQDE0SkFbgB+AGwSkQWAzuAKwFUdauIrAKexZ0RdK2qhqNS8RPjYuAzwBY/Fg/wd1j7ADQAK/zZOQGwSlUfFJG1WNsUc8r9u7FLeBhjjBmWDT0ZY4wZliUKY4wxw7JEYYwxZliWKIwxxgzLEoUxxphhWaIwxhgzLEsUxhhjhvX/A1RB9+sX+MgFAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True) \n",
        "\n",
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=0, callbacks=[callback])\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXfwUFjSZVRc"
      },
      "source": [
        "### Regularization\n",
        "\n",
        "Like other linear models, we can implement regularization to help tame overfitting. \n",
        "\n",
        "We can use both L2 (Ridge) regularization that will limit growth of coefficients, and L1 (Lasso) regularization that is able to eliminate features by shrinking their coefficients to 0. The functionality is the same as we are used to, a regularization term is added to the loss, and the optimization, such as gradient descent, is then performed as normal. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cgwwVgJ6ZVRd",
        "outputId": "7c4c3e58-3c0a-42ef-b502-764d87747a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_5 (Normalizat  (None, 18)               37        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 128)               2432      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 35,622\n",
            "Trainable params: 35,585\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Regularization\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(128, input_dim=18, activation='relu'))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=\"l1\"))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bnRVShe2ZVRd",
        "outputId": "fb5bbfba-efcd-4d12-ba31-427900c8da53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 0s 2ms/step - loss: 67908.7422\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=100, batch_size=100, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXboQ7N2ZVRd"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "Neural networks also commonly employ a technique call dropouts to prevent overfitting. This works just like the name says, every time the data is moved from one layer to another some portion of the features are randomly held out from being used. \n",
        "\n",
        "The intuitive explanation for dropout is that because individual nodes in the network cannot rely on the output of the others, each node must output features that are useful on their own. This sounds somewhat weird, but is actually effective. The number of features held out is called the dropout rate, typically between .2 and .5. \n",
        "\n",
        "An analogy can be drawn to the bootstrapping we looked at with trees - some random subset of features is selected each time, resulting in each batch getting \"a slightly different look at the data\", thus preventing overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gUJv1QPEZVRd",
        "outputId": "b6f5125b-b396-4dfd-9cd4-95d84f9c452e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_6 (Normalizat  (None, 18)               37        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 512)               9728      \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 798,246\n",
            "Trainable params: 798,209\n",
            "Non-trainable params: 37\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Dropout\n",
        "#Test Different Model Capacities\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(X_train))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(normalizer)\n",
        "model.add(Dense(512, input_dim=18, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EUJHDMXbZVRd",
        "outputId": "aff714cd-a820-49b3-8ede-e24737a275b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "169/169 [==============================] - 1s 4ms/step - loss: 73836.4141\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.compile(loss='mean_absolute_error', optimizer=tf.optimizers.Adam(learning_rate=.01))\n",
        "train_log = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_split=.2, verbose=0)\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AkcKFzNZVRd"
      },
      "source": [
        "### Predictions\n",
        "\n",
        "Once the model is trained, using it is mostly familiar to us from the sklearn stuff. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "S-jw_zjdZVRe",
        "outputId": "0a3fbf07-c78c-4bea-fa3e-652875330375"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "73836.41328182828"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = model.predict(X_test)\n",
        "mean_absolute_error(y_test, preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t9YyxGjZVRe"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Use the California data from previously and try to add some regularization things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGstyyzcZVRe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXYC8NGfZVRe"
      },
      "source": [
        "### Customized Loss\n",
        "\n",
        "Most scenarios are totally fine with a standard loss function, but what if we have something odd? What if we are playing on The Price is Right? We want to get as close as we can, without going over. We can write a loss function to mirror that!\n",
        "\n",
        "More practically, some real life scenarios have a disperse impact of different types of error. For example, if you are working for a call centre and predicting the number of agents to staff. Having slightly too many may be an error that costs a little bit of money, but not that big of a deal. Predicting too few might incur serious penalties if callers wait and you violate an SLA. Being off in one direction is bad, being off in the other direction can cause you to \"fall off of a cliff\" so to speak. In cases where the impact of the error is not uniform, custom loss functions may make sense. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GNjj1-aHZVRe"
      },
      "outputs": [],
      "source": [
        "def priceIsRight(y_true, y_pred):\n",
        "    if y_pred <= y_true:\n",
        "        return (y_true - y_pred) ** 2\n",
        "    else:\n",
        "        return (y_true - y_pred) ** 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyN6IYzbZVRe"
      },
      "source": [
        "## Optional Exercise\n",
        "\n",
        "Try to use the California data with a customized loss function. \n",
        "\n",
        "Note: this is a 20 way classification, so you'll probably want that many neurons on the output layer, an appropriate activation (softmax), and the y values will need to be run through np_utils.to_categorical. As well, think about the loss function, try categorical crossentropy.\n",
        "\n",
        "We'll look at activation and loss functions more next week. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAEhLMc6ZVRe"
      },
      "source": [
        "## Big Exercise - Newsgroup Classification\n",
        "\n",
        "Try to classify the newsgroup data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pCHlBNRQZVRe"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "remove = (\"headers\", \"footers\", \"quotes\")\n",
        "\n",
        "data_train = fetch_20newsgroups(\n",
        "    subset=\"train\", shuffle=True, remove=remove)\n",
        "\n",
        "data_test = fetch_20newsgroups(\n",
        "    subset=\"test\", shuffle=True, remove=remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1e1Iq4hZVRe",
        "outputId": "e2d62827-98eb-4a5b-87ea-e91475bd0b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (11314, 1971374)   Test: (7532, 1971374)\n"
          ]
        }
      ],
      "source": [
        "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
        "X_train = news_tf.fit_transform(data_train.data)\n",
        "y_train = data_train.target\n",
        "X_test = news_tf.transform(data_test.data)\n",
        "y_test = data_test.target\n",
        "print(\"Train:\", X_train.shape, \"  Test:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DIHW5SORZVRf"
      },
      "outputs": [],
      "source": [
        "y_test = np_utils.to_categorical(y_test)\n",
        "y_train = np_utils.to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "86ThuxsYZVRf"
      },
      "outputs": [],
      "source": [
        "in_size = 200\n",
        "tsvd = TruncatedSVD(n_components=in_size)\n",
        "X_train = tsvd.fit_transform(X_train)\n",
        "X_test = tsvd.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7H1YvGdZVRf",
        "outputId": "309f9e45-1f30-4b0f-e64e-a15cbd2bac41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_25 (Dense)            (None, 600)               120600    \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 600)               360600    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 600)               0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 600)               360600    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 600)               0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 400)               240400    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 400)               160400    \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 200)               80200     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 200)               40200     \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 20)                4020      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,367,020\n",
            "Trainable params: 1,367,020\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(in_size*3, input_dim=in_size, activation='relu'))\n",
        "model.add(Dense(in_size*3, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size*3, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size*2, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size*2, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(in_size, activation='relu'))\n",
        "model.add(Dense(20, activation=\"softmax\"))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t6JlBX_aZVRf",
        "outputId": "eee94ee4-6a24-4da5-9222-23f4a3decdc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "91/91 [==============================] - 3s 25ms/step - loss: 2.7407 - accuracy: 0.1035 - val_loss: 2.2761 - val_accuracy: 0.2121\n",
            "Epoch 2/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 1.9369 - accuracy: 0.3233 - val_loss: 1.7190 - val_accuracy: 0.4030\n",
            "Epoch 3/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 1.5482 - accuracy: 0.4723 - val_loss: 1.5526 - val_accuracy: 0.4755\n",
            "Epoch 4/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 1.3080 - accuracy: 0.5744 - val_loss: 1.3860 - val_accuracy: 0.5669\n",
            "Epoch 5/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 1.1776 - accuracy: 0.6216 - val_loss: 1.3144 - val_accuracy: 0.5992\n",
            "Epoch 6/100\n",
            "91/91 [==============================] - 2s 21ms/step - loss: 1.0515 - accuracy: 0.6652 - val_loss: 1.2605 - val_accuracy: 0.6191\n",
            "Epoch 7/100\n",
            "91/91 [==============================] - 2s 21ms/step - loss: 0.9815 - accuracy: 0.6955 - val_loss: 1.2793 - val_accuracy: 0.6275\n",
            "Epoch 8/100\n",
            "91/91 [==============================] - 2s 21ms/step - loss: 0.8764 - accuracy: 0.7272 - val_loss: 1.3134 - val_accuracy: 0.6310\n",
            "Epoch 9/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 0.7949 - accuracy: 0.7524 - val_loss: 1.2921 - val_accuracy: 0.6346\n",
            "Epoch 10/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 0.7467 - accuracy: 0.7683 - val_loss: 1.3449 - val_accuracy: 0.6376\n",
            "Epoch 11/100\n",
            "91/91 [==============================] - 2s 20ms/step - loss: 0.6600 - accuracy: 0.7902 - val_loss: 1.4307 - val_accuracy: 0.6354\n",
            "Epoch 12/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 0.6124 - accuracy: 0.8092 - val_loss: 1.3672 - val_accuracy: 0.6483\n",
            "Epoch 13/100\n",
            "91/91 [==============================] - 2s 21ms/step - loss: 0.5602 - accuracy: 0.8220 - val_loss: 1.4184 - val_accuracy: 0.6549\n",
            "Epoch 14/100\n",
            "91/91 [==============================] - 2s 21ms/step - loss: 0.5159 - accuracy: 0.8377 - val_loss: 1.5027 - val_accuracy: 0.6332\n",
            "Epoch 15/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 0.4946 - accuracy: 0.8450 - val_loss: 1.5455 - val_accuracy: 0.6310\n",
            "Epoch 16/100\n",
            "91/91 [==============================] - 2s 21ms/step - loss: 0.4598 - accuracy: 0.8554 - val_loss: 1.5910 - val_accuracy: 0.6478\n",
            "Epoch 17/100\n",
            "91/91 [==============================] - 2s 22ms/step - loss: 0.4267 - accuracy: 0.8653 - val_loss: 1.7610 - val_accuracy: 0.6469\n",
            "Epoch 18/100\n",
            "91/91 [==============================] - 2s 21ms/step - loss: 0.4035 - accuracy: 0.8754 - val_loss: 1.6407 - val_accuracy: 0.6443\n",
            "Epoch 19/100\n",
            "91/91 [==============================] - 2s 20ms/step - loss: 0.3546 - accuracy: 0.8869 - val_loss: 1.8708 - val_accuracy: 0.6394\n",
            "Epoch 20/100\n",
            "91/91 [==============================] - 2s 21ms/step - loss: 0.3694 - accuracy: 0.8821 - val_loss: 1.8860 - val_accuracy: 0.6465\n",
            "Epoch 21/100\n",
            "91/91 [==============================] - 2s 20ms/step - loss: 0.3298 - accuracy: 0.8988 - val_loss: 1.8436 - val_accuracy: 0.6399\n",
            "236/236 [==============================] - 1s 5ms/step - loss: 1.6135 - accuracy: 0.5809\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True) \n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
        "train_log = model.fit(X_train, y_train, epochs=100, batch_size=100, validation_split=.2, verbose=1, callbacks=[callback])\n",
        "model.evaluate(X_test, y_test)\n",
        "plot_loss(train_log)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "022_keras_tensor_sol.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('ml3950')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
